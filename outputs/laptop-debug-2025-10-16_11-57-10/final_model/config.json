{
  "architectures": [
    "StandardTransformerForCausalLM"
  ],
  "attention_dropout": 0.0,
  "attn_implementation": "eager",
  "beta_schedule": {
    "beta_ce_end": 100.0,
    "beta_ce_start": 0.1,
    "beta_cu_end": 100.0,
    "beta_cu_start": 0.1,
    "type": "cosine",
    "warmup_steps": 100
  },
  "capacity": 0.5,
  "from_scratch": true,
  "hidden_act": "silu",
  "hidden_size": 32,
  "initializer_range": 0.02,
  "intermediate_size": 128,
  "layer_types": [
    "full_attention",
    "full_attention"
  ],
  "learn_m_cu": true,
  "learn_o_ce": true,
  "m_cu_init": 1.1,
  "ma_window": 100,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "mod": {
    "aux_loss_weight": 0.01,
    "predictor_loss_weight": 0.01
  },
  "model_type": "standard",
  "num_attention_heads": 4,
  "num_hidden_layers": 2,
  "num_key_value_heads": 2,
  "o_ce_init": 1.025,
  "pretrained_model_name_or_path": "Qwen/Qwen2.5-10M",
  "prior_ffn_intermediate_size_factor": 0.0625,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "scratch_config": {
    "0.5B": {
      "hidden_size": 896,
      "intermediate_size": 4864,
      "num_attention_heads": 14,
      "num_hidden_layers": 24,
      "num_key_value_heads": 2
    },
    "10M": {
      "hidden_size": 32,
      "intermediate_size": 128,
      "num_attention_heads": 4,
      "num_hidden_layers": 2,
      "num_key_value_heads": 2
    },
    "max_position_embeddings": 32768,
    "rope_theta": 1000000.0,
    "sliding_window": 131072,
    "vocab_size": 151936
  },
  "sdt": {
    "causal_loss_weight": 0.01,
    "prior_loss_weight": 0.05
  },
  "size": "10M",
  "sliding_window": null,
  "stt": {
    "causal_router_loss_weight": 0.01,
    "g_reg_loss_weight": 0.001,
    "g_threshold": 0.75,
    "tpn_loss_weight": 0.05,
    "use_g_threshold_selection": true
  },
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "train_causal_router": true,
  "transformers_version": "4.54.1",
  "type": "standard",
  "use_cache": false,
  "use_causal_router_in_validation": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}