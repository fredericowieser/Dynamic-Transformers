model_type: dtf
training:
  num_epochs: 3
  max_steps: 1000
  gradient_accumulation_steps: 16
  gradient_clip_val: 1.0
  optimizer:
    lr: 0.0003
    weight_decay: 0.1
    warmup_ratio: 0.05
    scheduler: cosine_with_restarts
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1.0e-08
  lr_multipliers:
    base_model: 1.0
    router: 5.0
    prior: 5.0
  eval_interval: 100
  save_interval: 500
  eval_samples: 1000
  gradient_checkpointing: true
  mixed_precision: false
logging:
  wandb:
    enabled: true
    project: dynamic-transformer-mac
    entity: null
    tags:
    - mac
    - 50m
    - dtf
  log_interval: 50
  log_grad_norm: true
system:
  seed: 42
  output_dir: outputs/mac_dtf_20250917_224506
  device: mps
  num_workers: 4
  pin_memory: false
  compile: false
  compile_mode: null
dataloader:
  persistent_workers: false
  prefetch_factor: 2
  drop_last: true
checkpoint:
  save_total_limit: 3
  save_best: true
  metric: val_loss
  mode: min
early_stopping:
  enabled: true
  patience: 5
  min_delta: 0.001
data:
  data:
    mixed: true
    dataset_names:
    - tatsu-lab/alpaca
    - databricks/databricks-dolly-15k
    - teknium/openhermes
    - Open-Orca/OpenOrca
    - codeparrot/github-code-clean
    - bigcode/starcoderdata
    - c4
    - allenai/c4
    - openwebtext
    - bookcorpus
    - pg19
    - scientific_papers
    - allenai/s2orc
    - allenai/soda
    - daily_dialog
    - gsm8k
    - competition_math
    - wikipedia
    - wikihow
    dataset_weights:
    - 0.08
    - 0.07
    - 0.05
    - 0.05
    - 0.08
    - 0.07
    - 0.07
    - 0.06
    - 0.07
    - 0.05
    - 0.05
    - 0.05
    - 0.05
    - 0.025
    - 0.025
    - 0.025
    - 0.025
    - 0.07
    - 0.03
    block_size: 2048
    batch_size: 8
    num_workers: 4
    validation_split: 0.01
    add_special_tokens: true
    truncation: true
    padding: max_length
    min_length: 100
    max_length: 50000
    sampling_strategy: weighted
    seed: 42
model:
  model:
    pretrained_model_name: Qwen/Qwen2.5-0.5B
    num_hidden_layers: 12
    hidden_size: 512
    intermediate_size: 1536
    num_attention_heads: 8
    num_key_value_heads: 4
    max_position_embeddings: 2048
    rope_theta: 1000000.0
    rms_norm_eps: 1.0e-06
    vocab_size: 151936
    use_flash_attention: false
    torch_dtype: float32
    capacity_gamma: 0.5
    beta_ce_init: -0.5
    beta_cu_init: -0.8
    cu_detection_multiplier_init: 1.2
    ce_criterion_offset_init: 1.0
    prior_ffn_intermediate_size_factor: 0.25
    prior_loss_weight: 0.1
    init_std: 0.02
    tie_word_embeddings: true
