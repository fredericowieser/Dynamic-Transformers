{
  "arc_challenge": {
    "alias": "arc_challenge",
    "acc,none": 0.22525597269624573,
    "acc_stderr,none": 0.012207839995407317,
    "acc_norm,none": 0.2627986348122867,
    "acc_norm_stderr,none": 0.012862523175351333
  },
  "hellaswag": {
    "alias": "hellaswag",
    "acc,none": 0.30103565026887075,
    "acc_stderr,none": 0.0045777070250314035,
    "acc_norm,none": 0.3322047400916152,
    "acc_norm_stderr,none": 0.004700413824942549
  },
  "mmlu": {
    "acc,none": 0.24070645207235436,
    "acc_stderr,none": 0.003603131208819235,
    "alias": "mmlu"
  },
  "mmlu_humanities": {
    "acc,none": 0.23868225292242295,
    "acc_stderr,none": 0.006215361232528284,
    "alias": " - humanities"
  },
  "mmlu_formal_logic": {
    "alias": "  - formal_logic",
    "acc,none": 0.1984126984126984,
    "acc_stderr,none": 0.035670166752768614
  },
  "mmlu_high_school_european_history": {
    "alias": "  - high_school_european_history",
    "acc,none": 0.24242424242424243,
    "acc_stderr,none": 0.03346409881055953
  },
  "mmlu_high_school_us_history": {
    "alias": "  - high_school_us_history",
    "acc,none": 0.24509803921568626,
    "acc_stderr,none": 0.030190282453501954
  },
  "mmlu_high_school_world_history": {
    "alias": "  - high_school_world_history",
    "acc,none": 0.25738396624472576,
    "acc_stderr,none": 0.028458820991460285
  },
  "mmlu_international_law": {
    "alias": "  - international_law",
    "acc,none": 0.2644628099173554,
    "acc_stderr,none": 0.04026187527591206
  },
  "mmlu_jurisprudence": {
    "alias": "  - jurisprudence",
    "acc,none": 0.2222222222222222,
    "acc_stderr,none": 0.0401910747255735
  },
  "mmlu_logical_fallacies": {
    "alias": "  - logical_fallacies",
    "acc,none": 0.2392638036809816,
    "acc_stderr,none": 0.033519538795212696
  },
  "mmlu_moral_disputes": {
    "alias": "  - moral_disputes",
    "acc,none": 0.2398843930635838,
    "acc_stderr,none": 0.022989592543123563
  },
  "mmlu_moral_scenarios": {
    "alias": "  - moral_scenarios",
    "acc,none": 0.2335195530726257,
    "acc_stderr,none": 0.014149575348976266
  },
  "mmlu_philosophy": {
    "alias": "  - philosophy",
    "acc,none": 0.18006430868167203,
    "acc_stderr,none": 0.021823422857744953
  },
  "mmlu_prehistory": {
    "alias": "  - prehistory",
    "acc,none": 0.22839506172839505,
    "acc_stderr,none": 0.023358211840626267
  },
  "mmlu_professional_law": {
    "alias": "  - professional_law",
    "acc,none": 0.24771838331160365,
    "acc_stderr,none": 0.011025499291443738
  },
  "mmlu_world_religions": {
    "alias": "  - world_religions",
    "acc,none": 0.29239766081871343,
    "acc_stderr,none": 0.03488647713457923
  },
  "mmlu_other": {
    "acc,none": 0.2471837785645317,
    "acc_stderr,none": 0.007726499192394607,
    "alias": " - other"
  },
  "mmlu_business_ethics": {
    "alias": "  - business_ethics",
    "acc,none": 0.2,
    "acc_stderr,none": 0.04020151261036846
  },
  "mmlu_clinical_knowledge": {
    "alias": "  - clinical_knowledge",
    "acc,none": 0.21509433962264152,
    "acc_stderr,none": 0.025288394502891373
  },
  "mmlu_college_medicine": {
    "alias": "  - college_medicine",
    "acc,none": 0.19653179190751446,
    "acc_stderr,none": 0.030299574664788147
  },
  "mmlu_global_facts": {
    "alias": "  - global_facts",
    "acc,none": 0.2,
    "acc_stderr,none": 0.040201512610368445
  },
  "mmlu_human_aging": {
    "alias": "  - human_aging",
    "acc,none": 0.27802690582959644,
    "acc_stderr,none": 0.030069584874494043
  },
  "mmlu_management": {
    "alias": "  - management",
    "acc,none": 0.1650485436893204,
    "acc_stderr,none": 0.036756688322331886
  },
  "mmlu_marketing": {
    "alias": "  - marketing",
    "acc,none": 0.19658119658119658,
    "acc_stderr,none": 0.02603538609895129
  },
  "mmlu_medical_genetics": {
    "alias": "  - medical_genetics",
    "acc,none": 0.3,
    "acc_stderr,none": 0.046056618647183814
  },
  "mmlu_miscellaneous": {
    "alias": "  - miscellaneous",
    "acc,none": 0.26436781609195403,
    "acc_stderr,none": 0.015769984840690518
  },
  "mmlu_nutrition": {
    "alias": "  - nutrition",
    "acc,none": 0.24509803921568626,
    "acc_stderr,none": 0.02463004897982478
  },
  "mmlu_professional_accounting": {
    "alias": "  - professional_accounting",
    "acc,none": 0.25886524822695034,
    "acc_stderr,none": 0.026129572527180848
  },
  "mmlu_professional_medicine": {
    "alias": "  - professional_medicine",
    "acc,none": 0.3088235294117647,
    "acc_stderr,none": 0.02806499816704009
  },
  "mmlu_virology": {
    "alias": "  - virology",
    "acc,none": 0.25903614457831325,
    "acc_stderr,none": 0.03410646614071856
  },
  "mmlu_social_sciences": {
    "acc,none": 0.23041923951901203,
    "acc_stderr,none": 0.007579715658581327,
    "alias": " - social sciences"
  },
  "mmlu_econometrics": {
    "alias": "  - econometrics",
    "acc,none": 0.19298245614035087,
    "acc_stderr,none": 0.037124548537213684
  },
  "mmlu_high_school_geography": {
    "alias": "  - high_school_geography",
    "acc,none": 0.2727272727272727,
    "acc_stderr,none": 0.03173071239071724
  },
  "mmlu_high_school_government_and_politics": {
    "alias": "  - high_school_government_and_politics",
    "acc,none": 0.27461139896373055,
    "acc_stderr,none": 0.03221024508041154
  },
  "mmlu_high_school_macroeconomics": {
    "alias": "  - high_school_macroeconomics",
    "acc,none": 0.20512820512820512,
    "acc_stderr,none": 0.020473233173551975
  },
  "mmlu_high_school_microeconomics": {
    "alias": "  - high_school_microeconomics",
    "acc,none": 0.226890756302521,
    "acc_stderr,none": 0.027205371538279483
  },
  "mmlu_high_school_psychology": {
    "alias": "  - high_school_psychology",
    "acc,none": 0.1853211009174312,
    "acc_stderr,none": 0.016659279700295827
  },
  "mmlu_human_sexuality": {
    "alias": "  - human_sexuality",
    "acc,none": 0.29770992366412213,
    "acc_stderr,none": 0.04010358942462203
  },
  "mmlu_professional_psychology": {
    "alias": "  - professional_psychology",
    "acc,none": 0.22549019607843138,
    "acc_stderr,none": 0.016906615927288128
  },
  "mmlu_public_relations": {
    "alias": "  - public_relations",
    "acc,none": 0.2818181818181818,
    "acc_stderr,none": 0.0430911870994646
  },
  "mmlu_security_studies": {
    "alias": "  - security_studies",
    "acc,none": 0.2816326530612245,
    "acc_stderr,none": 0.028795185574291282
  },
  "mmlu_sociology": {
    "alias": "  - sociology",
    "acc,none": 0.22388059701492538,
    "acc_stderr,none": 0.02947525023601717
  },
  "mmlu_us_foreign_policy": {
    "alias": "  - us_foreign_policy",
    "acc,none": 0.23,
    "acc_stderr,none": 0.04229525846816505
  },
  "mmlu_stem": {
    "acc,none": 0.24738344433872503,
    "acc_stderr,none": 0.007667604120754329,
    "alias": " - stem"
  },
  "mmlu_abstract_algebra": {
    "alias": "  - abstract_algebra",
    "acc,none": 0.21,
    "acc_stderr,none": 0.040936018074033256
  },
  "mmlu_anatomy": {
    "alias": "  - anatomy",
    "acc,none": 0.31851851851851853,
    "acc_stderr,none": 0.040247784019771096
  },
  "mmlu_astronomy": {
    "alias": "  - astronomy",
    "acc,none": 0.2236842105263158,
    "acc_stderr,none": 0.03391160934343602
  },
  "mmlu_college_biology": {
    "alias": "  - college_biology",
    "acc,none": 0.22916666666666666,
    "acc_stderr,none": 0.03514697467862388
  },
  "mmlu_college_chemistry": {
    "alias": "  - college_chemistry",
    "acc,none": 0.22,
    "acc_stderr,none": 0.041633319989322695
  },
  "mmlu_college_computer_science": {
    "alias": "  - college_computer_science",
    "acc,none": 0.3,
    "acc_stderr,none": 0.046056618647183814
  },
  "mmlu_college_mathematics": {
    "alias": "  - college_mathematics",
    "acc,none": 0.23,
    "acc_stderr,none": 0.042295258468165065
  },
  "mmlu_college_physics": {
    "alias": "  - college_physics",
    "acc,none": 0.21568627450980393,
    "acc_stderr,none": 0.040925639582376556
  },
  "mmlu_computer_security": {
    "alias": "  - computer_security",
    "acc,none": 0.17,
    "acc_stderr,none": 0.03775251680686371
  },
  "mmlu_conceptual_physics": {
    "alias": "  - conceptual_physics",
    "acc,none": 0.2680851063829787,
    "acc_stderr,none": 0.02895734278834235
  },
  "mmlu_electrical_engineering": {
    "alias": "  - electrical_engineering",
    "acc,none": 0.20689655172413793,
    "acc_stderr,none": 0.03375672449560553
  },
  "mmlu_elementary_mathematics": {
    "alias": "  - elementary_mathematics",
    "acc,none": 0.21957671957671956,
    "acc_stderr,none": 0.021320018599770355
  },
  "mmlu_high_school_biology": {
    "alias": "  - high_school_biology",
    "acc,none": 0.26129032258064516,
    "acc_stderr,none": 0.024993053397764812
  },
  "mmlu_high_school_chemistry": {
    "alias": "  - high_school_chemistry",
    "acc,none": 0.22660098522167488,
    "acc_stderr,none": 0.029454863835292982
  },
  "mmlu_high_school_computer_science": {
    "alias": "  - high_school_computer_science",
    "acc,none": 0.25,
    "acc_stderr,none": 0.04351941398892446
  },
  "mmlu_high_school_mathematics": {
    "alias": "  - high_school_mathematics",
    "acc,none": 0.23703703703703705,
    "acc_stderr,none": 0.025928876132766107
  },
  "mmlu_high_school_physics": {
    "alias": "  - high_school_physics",
    "acc,none": 0.2052980132450331,
    "acc_stderr,none": 0.03297986648473834
  },
  "mmlu_high_school_statistics": {
    "alias": "  - high_school_statistics",
    "acc,none": 0.35185185185185186,
    "acc_stderr,none": 0.03256850570293647
  },
  "mmlu_machine_learning": {
    "alias": "  - machine_learning",
    "acc,none": 0.32142857142857145,
    "acc_stderr,none": 0.044328040552915185
  },
  "truthfulqa_mc2": {
    "alias": "truthfulqa_mc2",
    "acc,none": 0.45133267464851373,
    "acc_stderr,none": 0.015452188070573115
  },
  "winogrande": {
    "alias": "winogrande",
    "acc,none": 0.5035516969218626,
    "acc_stderr,none": 0.014052131146915869
  }
}