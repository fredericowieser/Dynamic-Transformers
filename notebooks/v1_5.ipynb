{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71b99b56",
      "metadata": {
        "id": "71b99b56"
      },
      "source": [
        "# Dynamic Transformer v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c6846c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33c6846c",
        "outputId": "1ee50e72-a039-42c9-d397-f88b6db05e50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c822419ad30>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Tuple, List\n",
        "from collections import deque\n",
        "import sys\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f764abc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f764abc9",
        "outputId": "4da6ffe6-47d8-440f-b267-85304c3c42cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# NORMAL HYPERPARAMETERS\n",
        "batch_size = 64 # How many independent sequences will we process in parallel?\n",
        "block_size = 256 # What is the maximum context length for predictions?\n",
        "max_iters = 6000 # The total number of training iterations.\n",
        "eval_interval = 500 # How often to evaluate the model's performance.\n",
        "learning_rate = 3e-4 # The step size for our optimizer.\n",
        "eval_iters = 25 # Number of batches to average for loss estimation.\n",
        "n_embd = 384 # The dimensionality of the token embeddings.\n",
        "n_head = 6 # The number of attention heads.\n",
        "n_layer = 6 # The number of transformer blocks.\n",
        "dropout = 0.2 # The probability of dropping out neurons during training.\n",
        "\n",
        "# DYNAMIC HYPERPARAMETERS\n",
        "dynamic_k = 0.9 # Surprise threshold multiplier for CU. Higher -> Less Updates\n",
        "d_st_history_window = 10000 # The number of past d_st values to average\n",
        "gate_loss_weight = 0 # The strength of the auxiliary loss on the gates\n",
        "gate_warmup_iters = 2500 # Iterations to encourage gates to open\n",
        "prior_loss_weight = 0.1 # The strength of the prior loss\n",
        "\n",
        "# Automatically select the best available device (CUDA, MPS, or CPU).\n",
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a559dc",
      "metadata": {
        "id": "72a559dc"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "We'll use the Tiny Shakespeare dataset. We first need to load the text and create a vocabulary of all unique characters. Then, we'll create functions to encode a string into a sequence of integers (tokens) and decode a sequence of tokens back into a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2a0f6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc2a0f6d",
        "outputId": "1e2b1395-83e7-48e3-ea66-68552402e1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-01 15:54:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘tiny-shakespeare.txt’\n",
            "\n",
            "tiny-shakespeare.tx 100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-07-01 15:54:35 (18.2 MB/s) - ‘tiny-shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You may need to download the data first\n",
        "!wget -O tiny-shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Find all unique characters in the text.\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create a mapping from characters to integers (stoi) and vice-versa (itos).\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # Encoder: takes a string, outputs a list of integers.\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: takes a list of integers, outputs a string.\n",
        "\n",
        "# Convert the entire dataset into a tensor of tokens.\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Split the data into training (90%) and validation (10%) sets.\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174f04fa",
      "metadata": {
        "id": "174f04fa"
      },
      "source": [
        "## Data Batching\n",
        "\n",
        "This function, `get_batch`, generates a small, random batch of data. For each sequence in the batch, the input `x` is a chunk of text, and the target `y` is the same chunk shifted by one character. This is how the model learns to predict the next character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4835543",
      "metadata": {
        "id": "c4835543"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # Select the appropriate dataset (train or val).\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Generate random starting indices for the batches.\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # Create the input sequences (x).\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    # Create the target sequences (y), which are shifted by one position.\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # Move the data to the selected device.\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a184bc41",
      "metadata": {
        "id": "a184bc41"
      },
      "source": [
        "## Loss Estimation\n",
        "\n",
        "To avoid noisy loss measurements, we estimate the loss by averaging it over multiple batches. This function is decorated with `@torch.no_grad()` to tell PyTorch not to calculate gradients, which saves memory and computation since we're only evaluating, not training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66202e09",
      "metadata": {
        "id": "66202e09"
      },
      "outputs": [],
      "source": [
        "# @torch.no_grad()\n",
        "# def estimate_loss():\n",
        "#     out = {}\n",
        "#     # Set the model to evaluation mode.\n",
        "#     model.eval()\n",
        "#     for split in ['train', 'val']:\n",
        "#         losses = torch.zeros(eval_iters)\n",
        "#         for k in range(eval_iters):\n",
        "#             X, Y = get_batch(split)\n",
        "#             logits, loss, _ = model(X, Y)\n",
        "#             losses[k] = loss.item()\n",
        "#         out[split] = losses.mean()\n",
        "#     # Set the model back to training mode.\n",
        "#     model.train()\n",
        "#     return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            # Pass a high iter number to ensure bias is off during eval\n",
        "            logits, loss, _ = model(X, Y, current_iter=max_iters) # MODIFIED\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b19ebac",
      "metadata": {
        "id": "6b19ebac"
      },
      "source": [
        "## The Transformer Model: A Deep Dive\n",
        "\n",
        "Now we'll build the Transformer model, piece by piece."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af87482a",
      "metadata": {
        "id": "af87482a"
      },
      "source": [
        "### Self-Attention Head\n",
        "\n",
        "Self-attention is the core mechanism of the Transformer. It allows tokens to interact with each other and weigh their importance. Each token produces a **Query** (what I'm looking for), a **Key** (what I contain), and a **Value** (what I'll communicate). The attention score is calculated by taking the dot product of a token's Query with every other token's Key. This score is then scaled and passed through a softmax function to get the weights. Finally, the output is a weighted sum of all tokens' Values.\n",
        "\n",
        "The mathematical formula is: $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e61dc4",
      "metadata": {
        "id": "b1e61dc4"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # Linear projections for Key, Query, and Value.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # A buffer for the triangular mask, not a model parameter.\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "\n",
        "        # Compute attention scores (\"affinities\").\n",
        "        # The dot product between queries and keys determines the attention weights.\n",
        "        # We scale by sqrt(d_k) to prevent the softmax from becoming too saturated.\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "\n",
        "        # Apply the causal mask to prevent tokens from attending to future tokens.\n",
        "        # This is crucial for a decoder-style language model.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        # Normalize the scores to get weights.\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Perform the weighted aggregation of the values.\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02cfccbc",
      "metadata": {
        "id": "02cfccbc"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "Instead of a single attention mechanism, Transformers use multiple attention \"heads\" in parallel. Each head can learn to focus on different types of relationships between tokens. The outputs of all heads are concatenated and projected back to the original embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74685c1e",
      "metadata": {
        "id": "74685c1e"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # Create a list of attention heads.\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # A linear layer to project the concatenated head outputs.\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate the outputs of each head along the last dimension.\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Project the result back to the embedding dimension.\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c158e90",
      "metadata": {
        "id": "8c158e90"
      },
      "source": [
        "### Feed-Forward Network\n",
        "\n",
        "After the attention mechanism gathers information, a simple feed-forward network processes this information for each token independently. It consists of two linear layers with a ReLU activation in between. This allows the model to perform more complex computations on the aggregated information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4346a8",
      "metadata": {
        "id": "af4346a8"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # The inner layer is typically 4x larger.\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # Project back to the embedding dimension.\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03d374a",
      "metadata": {
        "id": "d03d374a"
      },
      "source": [
        "### Dynamic Block\n",
        "\n",
        "A Dynamic block combines multi-head attention and two feed-forward networks one for the normal decoder functionality and one which acts as a prior on the probability of the model changing. It also includes two important features: residual connections and layer normalization.\n",
        "\n",
        "- **Residual Connections**: The input to a sub-layer (like attention) is added to its output. This helps prevent the vanishing gradient problem in deep networks.\n",
        "- **Layer Normalization**: This stabilizes the training by normalizing the features for each token across the embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9y6Hi6kEpLLh",
      "metadata": {
        "id": "9y6Hi6kEpLLh"
      },
      "outputs": [],
      "source": [
        "class DynamicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Parallel-aware dynamic transformer block.\n",
        "    All tensors are (B, T, C).  The first position (t=0) is forced open.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd: int, n_head: int):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        # Don't use acronyms\n",
        "        self.sa        = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd      = FeedFoward(n_embd)\n",
        "        self.prior_ffn = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ln3 = nn.LayerNorm(n_embd)\n",
        "        self.register_buffer(\"ones\", torch.tensor(1.0))  # for fast gate log\n",
        "        # Mathod 1: Have the MHA out saved\n",
        "        # self.prev_mha = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        current_iter: int = 0,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # (B,T,C)\n",
        "        mha_out  = x + self.sa(self.ln1(x))\n",
        "        posterior = mha_out + self.ffwd(self.ln2(mha_out))\n",
        "\n",
        "        # priors\n",
        "        prior_st = x\n",
        "        prev_mha = F.pad(mha_out[:, :-1, :], (0, 0, 1, 0))\n",
        "        prior_ch = self.prior_ffn(self.ln3(prev_mha))\n",
        "\n",
        "        # token surprises\n",
        "        d_st_tok = F.mse_loss(posterior, prior_st,\n",
        "                              reduction=\"none\").mean(-1) # (B,T)\n",
        "        d_ch_tok = F.mse_loss(posterior, prior_ch,\n",
        "                              reduction=\"none\").mean(-1) # (B,T)\n",
        "\n",
        "        # sequence-average surprises\n",
        "        D_st = d_st_tok.mean(dim=1) # (B,)\n",
        "        D_ch = d_ch_tok.mean(dim=1) # (B,)\n",
        "\n",
        "        # warm-up bias (scalar)\n",
        "        bias_scale = max(0.0, 1.0 - current_iter / gate_warmup_iters)\n",
        "        beta = D_ch.detach().mean() * bias_scale # scalar\n",
        "        D_ch_biased = D_ch + beta\n",
        "\n",
        "        # VPR decision (per sample)\n",
        "        CE = D_st > D_ch_biased # (B,) bool\n",
        "        CU = D_st > dynamic_k * D_st.detach().mean() # (B,) bool\n",
        "        gate_vec = (CE | CU).float() # (B,)\n",
        "\n",
        "        # mix whole block\n",
        "        gate = gate_vec.view(-1, 1, 1) # (B,1,1)\n",
        "        out = gate * posterior + (1.0 - gate) * x # (B,T,C)\n",
        "\n",
        "        # log single activation per block\n",
        "        self.last_gate = gate_vec.mean().detach() # scalar\n",
        "\n",
        "        # prediction-loss (detach posterior so gradients flow only into prior_ffn)\n",
        "        pred_loss = F.mse_loss(prior_ch, posterior.detach()) # scalar\n",
        "\n",
        "        return out, gate_vec, pred_loss            # gate is (B,T) – used for aux-loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fcd3d9",
      "metadata": {
        "id": "19fcd3d9"
      },
      "source": [
        "### Dynamic GPT Model\n",
        "\n",
        "Finally, we assemble all the components into the full GPT model. This includes:\n",
        "\n",
        "- **Token Embedding Table**: Converts input token indices into dense vectors (embeddings).\n",
        "- **Positional Embedding Table**: Since self-attention is permutation-invariant, we add positional embeddings to give the model information about the order of tokens.\n",
        "- **A Sequence of Transformer Blocks**: The core of the model where the processing happens.\n",
        "- **A Final Layer Norm and Linear Head**: To produce the final output logits over the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ua-U5w2xc7e",
      "metadata": {
        "id": "8ua-U5w2xc7e"
      },
      "outputs": [],
      "source": [
        "class DynamicGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DynamicBlock(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            idx,\n",
        "            targets=None,\n",
        "            current_iter: int = 0,\n",
        "        ):\n",
        "        B, T = idx.shape\n",
        "        tok = self.token_embedding_table(idx)\n",
        "        pos = self.position_embedding_table(\n",
        "            torch.arange(T, device=idx.device)\n",
        "        )\n",
        "        x = tok + pos                                   # (B,T,C)\n",
        "\n",
        "        gate_logs = [] # collect (B,T) per block\n",
        "        pred_losses = []\n",
        "        for block in self.blocks:\n",
        "            # x, mha_out, gate = block(x)\n",
        "            x, gate, pred_loss = block(x, current_iter=current_iter)\n",
        "            # gate_logs.append(gate\n",
        "            gate_logs.append(gate)\n",
        "            pred_losses.append(pred_loss)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                         # (B,T,V)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            lm_loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
        "            )\n",
        "\n",
        "            # gate activation loss\n",
        "            gate_tensor = torch.stack(gate_logs)\n",
        "            mean_gate_activation = gate_tensor.mean()\n",
        "            aux_gate = 1.0 - mean_gate_activation if current_iter < gate_warmup_iters \\\n",
        "                       else mean_gate_activation\n",
        "\n",
        "            # prediction loss\n",
        "            pred_loss_total = torch.stack(pred_losses).mean()\n",
        "\n",
        "            # combined objective\n",
        "            loss = (\n",
        "                lm_loss\n",
        "                + gate_loss_weight * aux_gate\n",
        "                + prior_loss_weight * pred_loss_total\n",
        "            )\n",
        "\n",
        "        # return per-block gate mean for live display\n",
        "        gate_means = torch.stack([g.mean() for g in gate_logs])  # (N,)\n",
        "        return logits, loss, gate_means\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # The generate function is now simpler, as it mirrors one step of the training loop.\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop the context to the last block_size tokens.\n",
        "            # This is the context the model will see.\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # Get the predictions. We pass a high current_iter to ensure\n",
        "            # the gate warm-up bias is turned off during generation.\n",
        "            # The forward method no longer takes a 'past_mhas' argument.\n",
        "            logits, loss, gate_means = self.forward(\n",
        "                idx_cond,\n",
        "                current_iter=max_iters,\n",
        "            )\n",
        "\n",
        "            # Focus only on the logit for the very last token in the sequence.\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "            # Apply softmax to get probabilities.\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "\n",
        "            # Sample from the distribution to get the next token.\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "\n",
        "            # Append the sampled index to the running sequence.\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae92f0a6",
      "metadata": {
        "id": "ae92f0a6"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Now we can instantiate the model and the optimizer. We'll use the AdamW optimizer, which is a standard choice for training Transformers. The training loop will repeatedly sample a batch of data, calculate the loss, and update the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f242d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2f242d8",
        "outputId": "37b204a9-090a-4d45-e4ad-c787ab2acc9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17.882945 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = DynamicGPT()\n",
        "activation_log = [[] for _ in range(len(model.blocks))]\n",
        "m = model.to(device)\n",
        "# Print the number of parameters in the model.\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# Create a PyTorch optimizer.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e419a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62e419a1",
        "outputId": "b4c6b9ee-1a09-4bad-f482-7c7d15a988e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/6000 [00:02<4:52:31,  2.93s/it, B0=1.00, B1=1.00, B2=1.00, B3=1.00, B4=1.00, B5=1.00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 0: train 4.3698 │ val 4.3678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 501/6000 [01:30<1:29:51,  1.02it/s, B0=1.00, B1=1.00, B2=1.00, B3=0.98, B4=0.98, B5=0.81]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 500: train 1.7987 │ val 1.9367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 1001/6000 [02:57<1:21:35,  1.02it/s, B0=1.00, B1=1.00, B2=0.98, B3=0.98, B4=0.97, B5=0.64]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 1000: train 1.4509 │ val 1.6592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 1501/6000 [04:25<1:13:24,  1.02it/s, B0=1.00, B1=1.00, B2=1.00, B3=0.98, B4=0.98, B5=0.66]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 1500: train 1.3135 │ val 1.5634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 2001/6000 [05:53<1:05:10,  1.02it/s, B0=1.00, B1=0.98, B2=1.00, B3=0.97, B4=0.92, B5=0.53]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 2000: train 1.2336 │ val 1.5373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 2501/6000 [07:20<57:01,  1.02it/s, B0=1.00, B1=1.00, B2=1.00, B3=0.95, B4=0.91, B5=0.58]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 2500: train 1.1825 │ val 1.5502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 3001/6000 [08:48<48:58,  1.02it/s, B0=1.00, B1=0.98, B2=1.00, B3=0.91, B4=0.84, B5=0.55]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 3000: train 1.1226 │ val 1.5215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 3501/6000 [10:15<40:38,  1.03it/s, B0=1.00, B1=0.91, B2=0.95, B3=0.83, B4=0.72, B5=0.47]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 3500: train 1.1006 │ val 1.5433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 4001/6000 [11:42<32:32,  1.02it/s, B0=1.00, B1=0.92, B2=0.97, B3=0.83, B4=0.75, B5=0.58]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 4000: train 1.0595 │ val 1.5490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 4501/6000 [13:10<24:24,  1.02it/s, B0=1.00, B1=0.94, B2=0.86, B3=0.88, B4=0.78, B5=0.53]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 4500: train 1.0217 │ val 1.5724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 5001/6000 [14:37<16:14,  1.02it/s, B0=1.00, B1=0.97, B2=0.83, B3=0.81, B4=0.72, B5=0.58]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 5000: train 0.9947 │ val 1.5878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 5501/6000 [16:04<08:07,  1.02it/s, B0=1.00, B1=0.91, B2=0.78, B3=0.73, B4=0.62, B5=0.52]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 5500: train 0.9483 │ val 1.6047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [17:32<00:00,  5.70it/s, B0=1.00, B1=0.98, B2=0.86, B3=0.78, B4=0.67, B5=0.53]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step 5999: train 0.9053 │ val 1.6380\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "pbar = tqdm(range(max_iters))\n",
        "for it in pbar:\n",
        "    if it % eval_interval == 0 or it == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"\\nstep {it}: train {losses['train']:.4f} │ val {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss, gate_means = model(xb, yb, current_iter=it)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # log & live display\n",
        "    for i, g in enumerate(gate_means):\n",
        "        activation_log[i].append(g.item())\n",
        "    pbar.set_postfix(\n",
        "        **{f\"B{i}\": f\"{g.item():.2f}\" for i, g in enumerate(gate_means)}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083acd83",
      "metadata": {
        "id": "083acd83"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "After training, we can use our model to generate new text. We start with a single token (a newline character in this case) and let the model predict the next token, which we then feed back into the model to predict the next one, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320d7d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "320d7d2d",
        "outputId": "26b76371-9ed0-44bf-ef5b-7634d716a0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Thy scant blocks sends, that kill'd my kindness lie.\n",
            "\n",
            "HASTINGS:\n",
            "Either stay on my son's patience,\n",
            "Go in my cousin, I'll pale.\n",
            "\n",
            "GLOUCESTER:\n",
            "Marry, that I may be find call'd to flatter me;\n",
            "For farely singly plain the way,\n",
            "No shallow thy right sleep; crave it my soul,\n",
            "Witness majesty a purnting cot;\n",
            "A vexation as a paranling one,\n",
            "To meet him dead. All seven or hands,\n",
            "Are your subjected by her:\n",
            "Who intending almost I am ost answer,\n",
            "He should not have slave counsel to me.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Gheen his \n"
          ]
        }
      ],
      "source": [
        "# Start generation with a single token (0 is the newline character).\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# Generate and decode the output.\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hz_gUvjFHSTE",
      "metadata": {
        "id": "Hz_gUvjFHSTE"
      },
      "source": [
        "## Analysis of Average Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tpfmbGlmHzQv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpfmbGlmHzQv",
        "outputId": "fffeadd4-28db-4ce3-a145-92ed18a7a337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Activation Rate Log ---\n",
            "   block_0_avg_act  block_1_avg_act  block_2_avg_act  block_3_avg_act  \\\n",
            "0         1.000000         1.000000         1.000000         1.000000   \n",
            "1         1.000000         1.000000         1.000000         1.000000   \n",
            "2         0.984375         0.984375         0.984375         0.984375   \n",
            "3         1.000000         1.000000         1.000000         1.000000   \n",
            "4         1.000000         1.000000         1.000000         1.000000   \n",
            "\n",
            "   block_4_avg_act  block_5_avg_act  \n",
            "0         1.000000         1.000000  \n",
            "1         1.000000         1.000000  \n",
            "2         0.984375         0.984375  \n",
            "3         1.000000         1.000000  \n",
            "4         1.000000         1.000000  \n",
            "\n",
            "--- Summary Statistics ---\n",
            "       block_0_avg_act  block_1_avg_act  block_2_avg_act  block_3_avg_act  \\\n",
            "count      6000.000000      6000.000000      6000.000000      6000.000000   \n",
            "mean          0.998070         0.962573         0.939419         0.891195   \n",
            "std           0.010731         0.032564         0.076197         0.085884   \n",
            "min           0.859375         0.812500         0.671875         0.625000   \n",
            "25%           1.000000         0.937500         0.890625         0.828125   \n",
            "50%           1.000000         0.968750         0.984375         0.906250   \n",
            "75%           1.000000         0.984375         1.000000         0.968750   \n",
            "max           1.000000         1.000000         1.000000         1.000000   \n",
            "\n",
            "       block_4_avg_act  block_5_avg_act  \n",
            "count      6000.000000      6000.000000  \n",
            "mean          0.835086         0.606497  \n",
            "std           0.120693         0.116998  \n",
            "min           0.515625         0.390625  \n",
            "25%           0.734375         0.531250  \n",
            "50%           0.843750         0.578125  \n",
            "75%           0.953125         0.625000  \n",
            "max           1.000000         1.000000  \n"
          ]
        }
      ],
      "source": [
        "# This cell works as-is, but the interpretation of the data changes.\n",
        "act_df = pd.DataFrame(activation_log).T\n",
        "act_df.columns = [f\"block_{i}_avg_act\" for i in range(len(model.blocks))]\n",
        "\n",
        "act_df.to_csv(\"block_activation_rates.csv\", index=False)\n",
        "\n",
        "print(\"--- Activation Rate Log ---\")\n",
        "print(act_df.head())\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(act_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BAPFNcFIxVkP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BAPFNcFIxVkP",
        "outputId": "33e0e6d2-e165-4cc5-8314-29c67909ecdf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"act_df\",\n  \"rows\": 6000,\n  \"fields\": [\n    {\n      \"column\": \"block_0_avg_act\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010730857016341959,\n        \"min\": 0.859375,\n        \"max\": 1.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.921875,\n          0.984375,\n          0.90625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_1_avg_act\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03256392249421758,\n        \"min\": 0.8125,\n        \"max\": 1.0,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.8125,\n          0.875,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_2_avg_act\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07619711206094652,\n        \"min\": 0.671875,\n        \"max\": 1.0,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          1.0,\n          0.796875,\n          0.890625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_3_avg_act\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08588426832331228,\n        \"min\": 0.625,\n        \"max\": 1.0,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.890625,\n          0.765625,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_4_avg_act\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1206933643697269,\n        \"min\": 0.515625,\n        \"max\": 1.0,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.546875,\n          0.796875,\n          0.625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_5_avg_act\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11699768720737429,\n        \"min\": 0.390625,\n        \"max\": 1.0,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.6875,\n          0.765625,\n          0.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "act_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-543f8e9f-3f31-4cd0-b82e-fb15ca48a56e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block_0_avg_act</th>\n",
              "      <th>block_1_avg_act</th>\n",
              "      <th>block_2_avg_act</th>\n",
              "      <th>block_3_avg_act</th>\n",
              "      <th>block_4_avg_act</th>\n",
              "      <th>block_5_avg_act</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.984375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.671875</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.531250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.546875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.796875</td>\n",
              "      <td>0.765625</td>\n",
              "      <td>0.671875</td>\n",
              "      <td>0.546875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.484375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>0.859375</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.671875</td>\n",
              "      <td>0.531250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-543f8e9f-3f31-4cd0-b82e-fb15ca48a56e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-543f8e9f-3f31-4cd0-b82e-fb15ca48a56e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-543f8e9f-3f31-4cd0-b82e-fb15ca48a56e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-694f42f0-5f27-405a-964f-370c3496ed55\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-694f42f0-5f27-405a-964f-370c3496ed55')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-694f42f0-5f27-405a-964f-370c3496ed55 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_30a645ac-6b6d-41c5-8a00-eefad90dec9a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('act_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_30a645ac-6b6d-41c5-8a00-eefad90dec9a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('act_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      block_0_avg_act  block_1_avg_act  block_2_avg_act  block_3_avg_act  \\\n",
              "0            1.000000         1.000000         1.000000         1.000000   \n",
              "1            1.000000         1.000000         1.000000         1.000000   \n",
              "2            0.984375         0.984375         0.984375         0.984375   \n",
              "3            1.000000         1.000000         1.000000         1.000000   \n",
              "4            1.000000         1.000000         1.000000         1.000000   \n",
              "...               ...              ...              ...              ...   \n",
              "5995         1.000000         0.937500         0.734375         0.671875   \n",
              "5996         1.000000         0.906250         0.734375         0.703125   \n",
              "5997         1.000000         0.984375         0.796875         0.765625   \n",
              "5998         1.000000         0.921875         0.781250         0.718750   \n",
              "5999         1.000000         0.984375         0.859375         0.781250   \n",
              "\n",
              "      block_4_avg_act  block_5_avg_act  \n",
              "0            1.000000         1.000000  \n",
              "1            1.000000         1.000000  \n",
              "2            0.984375         0.984375  \n",
              "3            1.000000         1.000000  \n",
              "4            1.000000         1.000000  \n",
              "...               ...              ...  \n",
              "5995         0.593750         0.531250  \n",
              "5996         0.562500         0.546875  \n",
              "5997         0.671875         0.546875  \n",
              "5998         0.656250         0.484375  \n",
              "5999         0.671875         0.531250  \n",
              "\n",
              "[6000 rows x 6 columns]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "act_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lwBmetLSxhtF",
      "metadata": {
        "id": "lwBmetLSxhtF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
