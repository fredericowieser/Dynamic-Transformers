# @package _global_

defaults:
  - _self_
  - system: default
  - model: default
  - data: default
  - training: default
  - logging: default

run:
  name: "qwen2.5-0.5B-tdtf-pretrain-${now:%Y-%m-%d_%H-%M-%S}"
  output_dir: "outputs/${run.name}"
  seed: 42
  run_final_evaluation: True

model:
  type: tdtf
  size: 0.5B
  attn_implementation: "flash_attention_2"

  # Required TDTF hyperparameters (no defaults in code)
  tpn_loss_weight: 0.05
  causal_loss_weight: 0.10
  tdtf_capacity: 0.50
  ma_window: 100
  o_ce_init: 1.025
  m_cu_init: 1.10
  prior_ffn_intermediate_size_factor: 0.0625
  student_routing_mode: "topk"  # or "threshold"

  router:
    beta_schedule:
      type: "cosine"           # "linear" or "cosine"
      beta_ce_start: 0.3       # start low (soft gating)
      beta_ce_end: 6.0         # end higher (hard gating)
      beta_cu_start: 0.3
      beta_cu_end: 6.0
      warmup_steps: 1000       # warmup before ramp

training:
  mode: finetune
  num_epochs: 1
  accumulate_grad_batches: 4
  max_steps: -1
  eval_interval: 100
  use_gradient_clipping: True
  gradient_clip_val: 1.0
  optimizer:
    base_model_lr: 1.0e-5
    tpn_router_lr: 1.0e-3
    causal_router_lr: 1.0e-2
    weight_decay: 0.01
    warmup_ratio: 0.01
    scheduler: cosine
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1.0e-8

system:
  device: cuda
  precision: bf16
  use_flash_attention: true
  compile_model: false
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  torch_dtype: bfloat16

logging:
  wandb:
    enabled: true
    project: "Dynamic-Transformers-Pretrain"
    entity: "huawei-noahs-ark"
    log_interval: 50

peft:
  enabled: false

push_to_hub:
  enabled: false

lm_eval:
  enabled: false
  tasks: ["arc_challenge", "hellaswag", "mmlu", "winogrande", "truthfulqa_mc2"]
  batch_size: 4
  merge_lora_for_eval: True