# @package _global_

training:
  mode: ???
  num_epochs: 10
  max_steps: -1
  accumulate_grad_batches: 64
  eval_interval: 10000
  use_gradient_clipping: False
  gradient_clip_val: 1.0
  gradient_checkpointing: True
  train_causal_router: True

  optimizer:
    scheduler: cosine
    weight_decay: 0.01
    warmup_ratio: 0.01
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8

    # --- NEW UNIFIED LR CONFIG ---
    # Default LR for any group not specified below
    lr: 1.0e-5

    # Learning rates per parameter group (must match names from get_trainable_parameters)
    lrs:
      base_model: 1.0e-5
      # MoD
      router: 1.0e-3
      # SDT
      prior: 1.0e-3
      # STT
      transition_network: 1.0e-3
      predictive_router: 1.0e-2
      # Common
      causal_router: 1.0e-2