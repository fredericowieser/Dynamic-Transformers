# @package _global_

model:
  size: 10M

  pretrained_model_name_or_path: null
  use_flash_attention_2: ${system.use_flash_attention}
  attn_implementation: "eager"
  use_cache: True
  tie_word_embeddings: True

  sdt_capacity: 0.5
  prior_loss_weight: 0.05
  causal_loss_weight: 0.01
  beta_ce_init: -0.3
  beta_cu_init: -0.6
  cu_detection_multiplier_init: 1.1
  ce_criterion_offset_init: 1.025
  prior_ffn_intermediate_size_factor: 0.5
  mod_capacity: 0.125
  mod_aux_loss_weight: 0.01
  tdtf_capacity: 0.5
  tpn_loss_weight: 0.05
  ma_window: 100
  o_ce_init: 1.025
  m_cu_init: 1.1
  scratch_config:
    vocab_size: 151936
    max_position_embeddings: 32768
    rope_theta: 1000000.0
    sliding_window: 131072
    10M:
      hidden_size: 32
      intermediate_size: 128
      num_hidden_layers: 2
      num_attention_heads: 4
      num_key_value_heads: 2
