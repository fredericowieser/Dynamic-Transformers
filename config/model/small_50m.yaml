# Configuration for ~50M parameter model
# Optimized for Mac training

model:
  # Base model - using smallest Qwen for initialization
  pretrained_model_name: "Qwen/Qwen2.5-0.5B"

  # Architecture modifications to get ~50M params
  num_hidden_layers: 12        # Reduce from 24 to 12
  hidden_size: 512             # Reduce from 896 to 512
  intermediate_size: 1536      # Reduce from 4864 to 1536
  num_attention_heads: 8       # Reduce from 14 to 8
  num_key_value_heads: 4       # Reduce for GQA

  # Keep these settings
  max_position_embeddings: 2048
  rope_theta: 1000000.0
  rms_norm_eps: 1e-6
  vocab_size: 151936  # Keep Qwen vocab

  # Mac optimizations
  use_flash_attention: false   # Flash attention may not work on Mac
  torch_dtype: "float32"       # Better stability on Mac

  # Dynamic model parameters
  capacity_gamma: 0.5          # Process 50% of tokens

  # DTF-specific parameters (well-tuned defaults)
  beta_ce_init: -0.5
  beta_cu_init: -0.8
  cu_detection_multiplier_init: 1.2
  ce_criterion_offset_init: 1.0
  prior_ffn_intermediate_size_factor: 0.25  # Smaller prior network
  prior_loss_weight: 0.1

  # Model initialization
  init_std: 0.02
  tie_word_embeddings: true