# Base training configuration
defaults:
  - _self_

model:
  type: ???  # Must be specified: standard, mod, or dtf
  size: 0.5B  # 0.5B, 1.5B, or 3B

training:
  from_scratch: ???  # Must be specified: true or false
  num_epochs: 3
  max_steps: -1
  gradient_accumulation_steps: 16
  gradient_clip_val: 1.0
  eval_interval: 500
  eval_samples: 1000
  save_interval: 1000
  optimizer:
    lr: 3e-4
    weight_decay: 0.1
    warmup_ratio: 0.05
    scheduler: cosine
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8

data:
  dataset: wikitext
  dataset_config: wikitext-2-raw-v1
  batch_size: 8
  block_size: 512
  validation_split: 0.05

logging:
  log_interval: 50

system:
  seed: 42
  output_dir: outputs/${model.type}_${model.size}_${now:%Y%m%d_%H%M%S}
  compile: false
