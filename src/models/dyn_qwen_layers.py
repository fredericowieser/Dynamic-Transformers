# src/models/d_qwen_layers.py

import torch
from torch import nn
from src.models.vpr_router import VPRRouter # Import the new VPRRouter

import logging
log = logging.getLogger(__name__)

class DynamicQwenDecoderLayer(nn.Module):
    """
    Implements the 'Dynamic Sub-Layer' for the Dynamic Qwen architecture.
    This layer is responsible for applying the VPR-based routing mechanism.
    It takes the outputs from a preceding DecisionQwenDecoderLayer and
    decides per-token (or per-batch) whether to keep the full computation
    output or to bypass it for computational savings.
    """
    def __init__(self, config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.hidden_size = config.hidden_size

        # The VPRRouter handles all the event detection and gating logic
        self.vpr_router = VPRRouter(config, layer_idx)

        # No traditional attention or MLP layers here; these are in DecisionQwenDecoderLayer.
        # This layer primarily performs the gating based on inputs from the Decision Layer.


    def forward(
        self,
        original_input_to_block: torch.Tensor, # Z^{n-1} (input to Decision Layer)
        posterior_full_path_output: torch.Tensor, # H^{D_n}_{trans} (output from Decision Layer's MLP)
        prior_hidden_states: torch.Tensor, # H^{D_n}_{prior} (output from Decision Layer's Prior FFN)
        attention_mask: torch.Tensor | None = None, # Passed through for compatibility, not used directly here
        position_ids: torch.LongTensor | None = None, # Passed through for compatibility, not used directly here
        past_key_values: tuple[torch.Tensor] | None = None, # Present_key_value from Decision Layer
        output_attentions: bool = False, # Attentions are handled and passed by Decision Layer
        use_cache: bool = False, # KV cache is generated by Decision Layer
        current_iter: int = 0, # Global training step for router's internal state/metrics (if any)
        **kwargs, # Accept extra kwargs, for future extensibility
    ) -> tuple[torch.Tensor, ...]:
        """
        Forward pass of the DynamicQwenDecoderLayer, performing VPR-based gating.

        Args:
            original_input_to_block (torch.Tensor): The original input hidden states (Z^{n-1})
                                                    to the *preceding Decision Layer*. (B, S, D)
            posterior_full_path_output (torch.Tensor): The output of the full Transformer computation
                                                       (Attention + MLP) from the *preceding Decision Layer*. (B, S, D)
            prior_hidden_states (torch.Tensor): The predicted prior hidden states from the
                                                Prior FFN in the *preceding Decision Layer*. (B, S, D)
            attention_mask, position_ids, past_key_values, output_attentions, use_cache:
                These are passed through from the main model's forward.
                The relevant values (present_key_value, attn_weights) from the
                DecisionQwenDecoderLayer should be passed as `past_key_values` and `attn_outputs`.
                Here we'll assume `past_key_values` are the new ones and `attn_outputs` are separate.
                Let's clarify: `DynamicQwenForCausalLM` will get `present_key_value` and `attn_weights`
                from the Decision Layer. This Dynamic Layer doesn't touch them; it just needs to
                pass them *back* as part of its output tuple for the `Qwen2ForCausalLM` structure.
                So `past_key_values` and `output_attentions` args should actually be the *outputs*
                from the Decision Layer that need to be re-passed to the top-level forward.
                Let's rename `past_key_values` to `decision_present_key_value` and
                `output_attentions` (as bool) to `decision_attn_weights` (as tensor or None).

            current_iter (int): Current global training iteration.
            kwargs: Placeholder for additional arguments (e.g., is_training).

        Returns:
            tuple:
                - hidden_states_final (torch.Tensor): The dynamically routed hidden states (Z^n). (B, S, D)
                - decision_attn_weights (torch.Tensor | None): Attention weights from the Decision Layer.
                - decision_present_key_value (tuple[torch.Tensor] | None): KV cache from the Decision Layer.
                - avg_ce_proportion (torch.Tensor): Scalar mean of CE activations from VPRRouter.
                - avg_cu_proportion (torch.Tensor): Scalar mean of CU activations from VPRRouter.
                - gate_vec_for_stats (torch.Tensor): The raw gate vector (B, S) or (B,) for logging.
        """
        # Ensure correct type for current_iter
        current_iter_tensor = torch.tensor(current_iter, device=posterior_full_path_output.device, dtype=torch.long)

        # Call the VPR Router to get the gating decision and metrics
        (
            gate_vec_final, # (B, T) or (B,)
            avg_ce_proportion,
            avg_cu_proportion,
            d_st_tok, # For potential future logging/analysis, but not returned by layer
            d_ch_tok, # For potential future logging/analysis, but not returned by layer
            combined_gating_signal, # For potential future logging/analysis, but not returned by layer
        ) = self.vpr_router(
            original_input_to_block=original_input_to_block,
            posterior_full_path_output=posterior_full_path_output,
            prior_hidden_states=prior_hidden_states,
            capacity_gamma=self.config.capacity_gamma, # Use the new capacity_gamma from config
            is_training=self.training, # Use PyTorch Lightning's training flag
        )

        if self.vpr_router.token_wise_gating:
            # gate_vec_final: (B, T) -> (B, T, 1) for element-wise multiplication
            gate = gate_vec_final.unsqueeze(-1)
        else:
            # gate_vec_final: (B,) -> (B, 1, 1) for broadcasting across sequence length
            gate = gate_vec_final.view(-1, 1, 1)

        # Apply the dynamic gating: gate*compute_path + (1-gate)*identity_path
        # The 'compute_path' is `posterior_full_path_output` from the Decision Layer
        # The 'identity_path' is `original_input_to_block` from the Decision Layer (its input)
        hidden_states_final = gate * posterior_full_path_output + (1.0 - gate) * original_input_to_block

        # Prepare outputs to match expected signature for DynamicQwenForCausalLM's loop
        # The elements `past_key_values` and `output_attentions` passed to this layer
        # are actually the `present_key_value` and `attn_weights` from the Decision Layer.
        # We need to return them as they were.
        outputs = (hidden_states_final,)

        # These are conditionally added based on whether they were requested AND generated by Decision Layer
        # The original Qwen2DecoderLayer forward output: hidden_states, present_key_value, attentions
        # Our DecisionLayer output: original_input, posterior_full_path_output, prior_hidden_states, present_key_value, attn_weights
        # The `past_key_values` argument to this layer effectively *is* `decision_present_key_value`
        # and `output_attentions` (boolean) signals if `attn_outputs` (actual tensor) is available
        # if output_attentions: outputs += (attn_outputs[1],) -> (hidden_states_final, attn_weights)
        # if use_cache: outputs += (attn_outputs[2],) -> (hidden_states_final, present_key_value) or (hidden_states_final, attn_weights, present_key_value)

        # To align with the standard Transformer layer output (hidden_states, attentions, present_key_value),
        # we need to ensure these are passed back. The `forward` signature for this layer
        # should reflect receiving these from the Decision Layer.
        # Let's adjust this layer's forward signature in `DynamicQwenForCausalLM` to pass these explicitly.
        # For *this* layer's return, let's keep it simple: `(hidden_states_final, present_key_value, attn_weights)`
        # followed by our custom metrics.

        # The `past_key_values` argument of this layer *is* the `present_key_value` from the decision layer.
        if use_cache: # This flag means Decision Layer produced cache, so we should return it
            outputs += (past_key_values,)

        if output_attentions: # This flag means Decision Layer produced attentions, so we should return them
            # We need to assume that `kwargs` contains the actual attention weights
            # or pass them directly as an arg. Let's make it an explicit argument.
            # Reworking the `DynamicQwenForCausalLM.forward` for this will be crucial.
            # For now, if output_attentions is True, we assume decision_attn_weights is passed in kwargs.
            attn_weights_from_decision = kwargs.get("decision_attn_weights", None)
            outputs += (attn_weights_from_decision,)


        # Append dynamic gating metrics
        # The order of these custom metrics MUST match what `DynamicQwenForCausalLM.forward` expects
        # (avg_ce_proportion, avg_cu_proportion, gate_vec_for_stats)
        return outputs + (avg_ce_proportion, avg_cu_proportion, gate_vec_final)