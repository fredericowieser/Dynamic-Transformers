defaults:
  - _self_
  - data: sft_mix

run:
  name: "llama3.2-1b-dynamic-finetune-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-dynamic_k=${model.model_cfg.dynamic_k}-ce_bias=${model.model_cfg.ce_bias}-lora_main=${model.model_cfg.lora.enable_lora_main_path}-lora_prior=${model.model_cfg.lora.enable_lora_prior_ffn}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto" # "cpu", "cuda", "mps", etc.
  precision: "bf16-mixed" # "32-true", "16-mixed", "bf16-mixed"
  run_final_evaluation: true

data:
  name: ${hydra:runtime.choices.data}

model:
  _target_: src.trainers.d_llama_trainer.DynamicLlamaTrainer
  model_cfg:
    model_name: "meta-llama/Llama-3.2-1B-instruct"
    prior_loss_weight: 0.1
    # CU = D_st > dynamic_k * D_st.detach().mean() # (B,) bool
    dynamic_k: 0.95
    token_wise: true # Use token-wise dynamic gating
    # CE = D_st > D_ch - ce_bias # (B,) bool
    ce_bias: 0.05

    init_prior_from_mlp: true 

    lora:
      enable_lora_main_path: true
      enable_lora_prior_ffn: true
      r: 8                    # LoRA attention dimension
      lora_alpha: 16          # LoRA scaling factor
      lora_dropout: 0.05      # Dropout probability for LoRA layers
      bias: "none"            # Type of bias to add ("none", "all", "lora_only")
      # Target modules for the main decoder path (LlamaAttention, LlamaMLP)
      lora_target_modules_main: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      # Target modules for the prior FFN (FeedForward class)
      lora_target_modules_prior_ffn: ["w1", "w2", "w3"]

training:
  accumulate_grad_batches: 64
  max_iters: 100000
  eval_interval: 200
  
  optimizer:
    base_lr: 1.0e-4       # Max LR for base model
    prior_ffn_lr: 1.0e-2  # Max LR for prior FFN
    weight_decay: 0.01

  scheduler:
    factor: 0.1           
    patience: 10

  gate_warmup_iters: 100

logging:
  tensorboard:
    enabled: true
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"