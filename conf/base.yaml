defaults:
  - _self_
  - data: sft_mix

run:
  name: "qwen2.5-0.5B-${model.model_cfg.dynamic_architecture}-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-gamma=${model.model_cfg.capacity_gamma}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto"
  precision: "bf16"
  run_final_evaluation: True

data:
  name: ${hydra:runtime.choices.data}
  batch_size: 6
  tokenizer_name: "Qwen/Qwen2.5-0.5B-Instruct"
  block_size: 1024
  validation_split_percentage: 2

peft:
  enabled: False
  config:
    _target_: peft.LoraConfig
    r: 16
    lora_alpha: 32
    target_modules:
      - "q_proj"      # Attention: Query projection
      - "k_proj"      # Attention: Key projection
      - "v_proj"      # Attention: Value projection
      - "o_proj"      # Attention: Output projection
      - "gate_proj"   # MLP: Gate projection for SwiGLU
      - "up_proj"     # MLP: Up projection for SwiGLU
      - "down_proj"   # MLP: Down projection
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"

model:
  _target_: src.models.qwen.causal_lm.DynamicQwenForCausalLM.from_pretrained
  pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"

  model_cfg:
    # --- Architecture Control ---
    dynamic_architecture: "vpr"  # Can be "vpr" or "mod"
    capacity_gamma: 0.5          # For VPR: routing capacity. For MoD: percentage of tokens to process.

    # --- VPR Specific Params ---
    prior_loss_schedule:
      initial_weight: 10.0 # Start with a high weight for aggressive initial training
      final_weight: 0.1   # Decay to a stable final weight
      decay_steps: 50   # Number of steps over which to perform the decay

    learn_beta_ce: False
    learn_beta_cu: False
    learn_cu_multiplier: False
    learn_ce_offset: True

    beta_ce_init: 0.001
    beta_cu_init: 0.001
    cu_detection_multiplier_init: 0.95
    ce_criterion_offset_init: 1.825

    token_wise_gating: True
    moving_average_window_size: 100
    prior_ffn_intermediate_size_factor: 0.0625  # 1/16th of the main FFN size

    # --- General Training Params ---
    freeze_main_transformer_blocks: False

training:
  accumulate_grad_batches: 16
  num_epochs: 2
  eval_interval: 1000

  use_gradient_clipping: False
  gradient_clip_val: 1.0

  optimizer:
    base_lr: 1.0e-5
    weight_decay: 0.001
    warmup_ratio: 0.01  

logging:
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"
