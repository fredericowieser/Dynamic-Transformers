defaults:
  - _self_
  - data: sft_mix

run:
  name: "qwen2.5-0.5B-${model.model_cfg.dynamic_architecture}-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-gamma=${model.model_cfg.capacity_gamma}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto"
  precision: "bf16"
  run_final_evaluation: true

data:
  name: ${hydra:runtime.choices.data}

model:
  _target_: src.models.qwen.causal_lm.DynamicQwenForCausalLM.from_pretrained
  pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"

  model_cfg:
    # --- Architecture Control ---
    dynamic_architecture: "vpr"  # Can be "vpr" or "mod"
    capacity_gamma: 0.5          # For VPR: routing capacity. For MoD: percentage of tokens to process.

    # --- VPR Specific Params ---
    freeze_vpr_router: True
    beta_ce_init: 0.01
    beta_cu_init: 0.01
    cu_detection_multiplier_init: 0.95
    ce_criterion_offset_init: 0.075
    token_wise_gating: True
    moving_average_window_size: 100
    prior_ffn_intermediate_size_factor: 0.125

    # --- General Training Params ---
    freeze_main_transformer_blocks: False

training:
  accumulate_grad_batches: 8
  num_epochs: 1
  eval_interval: 1000
  gradient_clip_val: 1.0

  optimizer:
    base_lr: 1.0e-3
    weight_decay: 0.01
    warmup_ratio: 0.01  

logging:
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"
