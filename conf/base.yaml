defaults:
  - _self_
  - data: pretrain_mix

run:
  name: "LARGER-PRIOR-TEST-qwen2.5-0.5B-${model.model_cfg.dynamic_architecture}-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-gamma=${model.model_cfg.capacity_gamma}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto"
  precision: "bf16"
  run_final_evaluation: True

data:
  name: ${hydra:runtime.choices.data}
  batch_size: 16
  tokenizer_name: "Qwen/Qwen2.5-0.5B"
  block_size: 1024
  validation_split_percentage: 2

peft:
  enabled: False
  config:
    _target_: peft.LoraConfig
    r: 16
    lora_alpha: 32
    target_modules:
      - "q_proj"      # Attention: Query projection
      - "k_proj"      # Attention: Key projection
      - "v_proj"      # Attention: Value projection
      - "o_proj"      # Attention: Output projection
      - "gate_proj"   # MLP: Gate projection for SwiGLU
      - "up_proj"     # MLP: Up projection for SwiGLU
      - "down_proj"   # MLP: Down projection
    modules_to_save:
      - "vpr_router"
      - "prior_ffn"
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"

model:
  _target_: src.models.qwen.causal_lm.DynamicQwenForCausalLM.from_pretrained
  pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B"
  use_flash_attention_2: true

  model_cfg:
    # --- Architecture Control ---
    dynamic_architecture: "vpr"  # Can be "vpr" or "mod"
    capacity_gamma: 0.5          # For VPR: routing capacity. For MoD: percentage of tokens to process.

    # --- VPR Specific Params ---
    prior_loss_schedule:
      initial_weight: 0.05 # Start with a high weight for aggressive initial training
      final_weight: 0.05   # Decay to a stable final weight
      decay_steps: 0   # Number of steps over which to perform the decay

    learn_beta_ce: True
    learn_beta_cu: True
    learn_cu_multiplier: True
    learn_ce_offset: True

    beta_ce_init: -0.3
    beta_cu_init: -0.6
    cu_detection_multiplier_init: 1.1
    ce_criterion_offset_init: 1.025

    token_wise_gating: True
    moving_average_window_size: 100
    prior_ffn_intermediate_size_factor: 0.125  # 1/8th of main FFN size

    # --- General Training Params ---
    freeze_main_transformer_blocks: False

training:
  # --- Training Duration Control ---
  # Specify EITHER num_epochs OR max_steps.
  # If max_steps is set to a positive number, it will take precedence.
  num_epochs: 1
  max_steps: -1

  accumulate_grad_batches: 64
  eval_interval: 100

  use_gradient_clipping: False
  gradient_clip_val: 1.0

  optimizer:
    base_model_lr: 1.0e-5       # base Qwen2Block parameters
    prior_lr: 1.0e-3
    vpr_router_lr: 1.0e-2
    weight_decay: 0.01
    warmup_ratio: 0.01

logging:
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"
