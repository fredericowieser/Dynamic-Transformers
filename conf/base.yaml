defaults:
  - _self_
  - data: sft_mix

run:
  name: "qwen2.5-0.5B-${model.model_cfg.dynamic_architecture}-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-gamma=${model.model_cfg.capacity_gamma}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto"
  precision: "bf16-mixed"
  run_final_evaluation: true

data:
  name: ${hydra:runtime.choices.data}

model:
  _target_: src.trainers.qwen_trainer.DynamicQwenTrainer
  model_cfg:
    model_name: "Qwen/Qwen2.5-0.5B-Instruct"

    # --- Architecture Control ---
    dynamic_architecture: "vpr"  # Can be "vpr" or "mod"
    capacity_gamma: 0.5          # For VPR: routing capacity. For MoD: percentage of tokens to process.

    # --- VPR Specific Params ---
    beta_ce_init: 0.01
    beta_cu_init: 0.01
    cu_detection_multiplier_init: 1.0
    ce_criterion_offset_init: 0.075
    token_wise_gating: True
    moving_average_window_size: 100
    prior_ffn_intermediate_size_factor: 0.5

    # --- General Training Params ---
    freeze_main_transformer_blocks: False

training:
  accumulate_grad_batches: 64
  max_iters: 100000
  eval_interval: 1000

  optimizer:
    base_lr: 1.0e-3 # Max LR for the scheduler
    weight_decay: 0.01
    warmup_ratio: 0.01  

  scheduler:
    factor: 0.1
    patience: 10

logging:
  tensorboard:
    enabled: true
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"