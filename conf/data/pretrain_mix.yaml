_target_: src.data.mixed_dataset.MixedDataset
_recursive_: false
_convert_: partial

dataset_configs:

  # Category 1: Foundational General Knowledge (~40%)
  # Diverse high-quality text to prevent catastrophic forgetting
  # and maintain broad language skills

  - type: "pretrain"
    dataset_name: "wikitext"
    dataset_config: "wikitext-103-raw-v1"
    text_column: "text"
    train_subset_ratio: 1.0  # ~1.8M docs, core text

  - type: "pretrain"
    dataset_name: "cnn_dailymail"
    dataset_config: "3.0.0"
    text_column: "article"
    train_subset_ratio: 0.2  # ~57k docs, news articles

  - type: "pretrain"
    dataset_name: "storytracer/US-PD-Books"
    text_column: "text"
    train_subset_ratio: 0.5  # ~327k docs, classic literature

  # Category 2: Academic & Scientific Reasoning (~40%)
  # Targets MMLU and ARC-Challenge with technical text

  - type: "pretrain"
    dataset_name: "HuggingFaceTB/cosmopedia"
    dataset_config: "openstax"  # OpenStax configuration
    text_column: "text"
    train_subset_ratio: 0.1  # ~3M docs, synthetic textbooks

  - type: "pretrain"
    dataset_name: "sciq"
    text_column: "support"
    train_subset_ratio: 1.0  # ~13k docs, science passages

  # Category 3: Logical & Commonsense Reasoning (~20%)
  # Improves Hellaswag and general reasoning performance

  - type: "pretrain"
    dataset_name: "codeparrot/codeparrot-clean-valid"
    text_column: "content"
    train_subset_ratio: 1.0  # ~18k docs, code structure

  - type: "pretrain"
    dataset_name: "roneneldan/TinyStories"
    text_column: "text"
    train_subset_ratio: 1.0  # ~2.1M docs, basic causality
