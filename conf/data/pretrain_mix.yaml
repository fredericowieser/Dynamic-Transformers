_target_: src.data.mixed_dataset.MixedDataset
_recursive_: false
_convert_: partial

dataset_configs:

  # === Category 1: Foundational General Text (~60%) ===
  # The backbone of the training mix. High-quality, diverse text to
  # prevent catastrophic forgetting and maintain broad language skills.

  - type: "pretrain"
    dataset_name: "wikitext"
    dataset_config: "wikitext-103-raw-v1"
    text_column: "text"
    train_subset_ratio: 1.0 # Use all of this high-quality corpus.

  - type: "pretrain"
    dataset_name: "openwebtext" # REPLACED: Switched to a stable, high-quality web text corpus.
    text_column: "text"
    # A large, open-source recreation of the WebText dataset. Excellent for
    # general-purpose language modeling.
    train_subset_ratio: 0.2 # Use a 20% slice to keep it manageable but substantial.

  # === Category 2: Logical & Scientific Reasoning (~30%) ===
  # Targeted datasets to boost performance on technical benchmarks.

  - type: "pretrain"
    dataset_name: "codeparrot/codeparrot-clean-valid"
    text_column: "content"
    # High-quality Python code teaches logical structure and reasoning,
    # which is crucial for benchmarks like BBH and MMLU.
    train_subset_ratio: 1.0

  - type: "pretrain"
    dataset_name: "sciq"
    text_column: "support"
    # Factual scientific text to ground the model for ARC-Challenge and MMLU.
    train_subset_ratio: 1.0

  # === Category 3: Causal Commonsense (~10%) ===
  # A small but powerful dataset for basic reasoning.

  - type: "pretrain"
    dataset_name: "roneneldan/TinyStories"
    text_column: "text"
    # Proven to be highly effective at teaching causal links and narrative.
    train_subset_ratio: 1.0