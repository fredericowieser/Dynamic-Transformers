_target_: src.data.mixed_datamodule.MixedDataModule  # Updated: New path to MixedDataModule
_recursive_: false   # <- DO NOT instantiate children automatically
_convert_:  partial  # <- Pass nested lists/dicts as DictConfig/ListConfig

dataset_configs:

  # === Category 1: Foundational Knowledge & Long-Form Coherence (~50%) ===
  # The bedrock of the model's understanding of language, facts, and narrative.

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "allenai/c4"
    dataset_config: "en"
    text_column: "text"
    # A large portion of C4 provides a high-quality, cleaned web text foundation.
    train_subset_ratio: 0.20

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "wikimedia/wikipedia"
    dataset_config: "20231101.en"
    text_column: "text"
    # The entirety of English Wikipedia is essential for high-quality, factual knowledge.
    train_subset_ratio: 1.0

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "the_pile_books3"
    text_column: "text"
    # Full book corpus for long-range coherence, narrative understanding, and style.
    train_subset_ratio: 1.0


  # === Category 2: Code & Technical Mastery (~20%) ===
  # To excel on HumanEval and MBPP, we need a strong dose of high-quality code.

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "bigcode/the-stack-dedup"
    dataset_config: "default"
    # Data from multiple high-value programming languages. This is the gold standard.
    data_files:
      - "data/python/*.parquet"
      - "data/java/*.parquet"
      - "data/javascript/*.parquet"
    text_column: "content"
    train_subset_ratio: 0.15 # 15% of this massive, deduplicated dataset is a huge amount of code.


  # === Category 3: Mathematical & Scientific Reasoning (~20%) ===
  # This is critical for boosting scores on MATH, GSM8K, and reasoning tasks.

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "open-web-math/open-web-math"
    text_column: "text"
    # Essential for mathematical reasoning. This contains high-quality web text with LaTeX.
    train_subset_ratio: 1.0

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "togethercomputer/RedPajama-Data-1T"
    dataset_config: "arxiv"
    text_column: "text"
    # We significantly increase the proportion of ArXiv papers for scientific knowledge and formal logic.
    train_subset_ratio: 0.60

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "wikimedia/wikibooks"
    dataset_config: "20231101.en"
    text_column: "text"
    # Wikibooks contains well-structured textbook content, great for math and science.
    train_subset_ratio: 1.0


  # === Category 4: Multilingual Foundation (~10%) ===
  # To address the multilingual benchmarks directly.

  - _target_: src.data.huggingface_dataset.HuggingFaceDataset
    dataset_name: "allenai/c4"
    # We sample from a wide range of languages from the multilingual C4 corpus.
    # Add or remove languages as needed.
    dataset_config: "multilingual"
    data_files:
      - "de/*.json.gz" # German
      - "fr/*.json.gz" # French
      - "es/*.json.gz" # Spanish
      - "zh/*.json.gz" # Chinese
      - "ar/*.json.gz" # Arabic
      - "ja/*.json.gz" # Japanese
    text_column: "text"
    train_subset_ratio: 0.10 # 10% of the data for each of these languages.