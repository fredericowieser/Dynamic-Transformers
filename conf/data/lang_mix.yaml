# This file defines a master mix for continued pre-training.
# The goal is to enrich the model's core knowledge with a diverse diet of
# high-quality text, including web data, books, code, and academic papers.
#
# To use this mix, run:
# python main.py data=lang_mix

_target_: src.core.data.MixedDataModule

# --- Overall parameters for the language mix ---
batch_size: 4
tokenizer_name: "meta-llama/Llama-3.2-1B-instruct"
block_size: 2048
validation_split_percentage: 1 # 1% is plenty for validation on these huge datasets.

# --- List of dataset configurations to mix ---
# We use different subsets of the RedPajama-Data-1T dataset, which is a
# high-quality reproduction of the original LLaMA training data.
# The subset ratios are small because the absolute size of these datasets is enormous.
dataset_configs:
  # 1. General Web Text (Foundation)
  - _target_: src.core.data.HuggingFaceDataModule
    dataset_name: "togethercomputer/RedPajama-Data-1T"
    dataset_config: "common_crawl"
    text_column: "text"
    train_subset_ratio: 0.01 # 1% of Common Crawl is still millions of documents.

  # 2. Long-form Coherence and Narrative
  - _target_: src.core.data.HuggingFaceDataModule
    dataset_name: "togethercomputer/RedPajama-Data-1T"
    dataset_config: "book"
    text_column: "text"
    train_subset_ratio: 0.05 # 5% of the books data.

  # 3. Foundational Code Understanding
  - _target_: src.core.data.HuggingFaceDataModule
    dataset_name: "togethercomputer/RedPajama-Data-1T"
    dataset_config: "github"
    text_column: "text"
    train_subset_ratio: 0.05 # 5% of the GitHub data.

  # 4. Technical and Academic Language
  - _target_: src.core.data.HuggingFaceDataModule
    dataset_name: "togethercomputer/RedPajama-Data-1T"
    dataset_config: "arxiv"
    text_column: "text"
    train_subset_ratio: 0.10 # 10% of ArXiv papers.

  # 5. Conversational and Q&A Style
  - _target_: src.core.data.HuggingFaceDataModule
    dataset_name: "togethercomputer/RedPajama-Data-1T"
    dataset_config: "stackexchange"
    text_column: "text"
    train_subset_ratio: 0.10 # 10% of StackExchange.