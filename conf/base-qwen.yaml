defaults:
  - _self_
  - data: sft_mix_qwen

run:
  name: "qwen2.5-0.5B-dynamic-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-dynk=${model.model_cfg.dynamic_k}-ceb=${model.model_cfg.ce_bias}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto"           # "cpu", "cuda", "mps", etc.
  precision: "bf16-mixed"  # "32-true", "16-mixed", "bf16-mixed"
  run_final_evaluation: true

data:
  name: ${hydra:runtime.choices.data}

model:
  _target_: src.trainers.d_qwen_trainer.DynamicQwenTrainer
  model_cfg:
    model_name: "Qwen/Qwen2.5-0.5B-Instruct"

    # --- START OF CHANGE ---
    prior_loss_weight: 0.1 # Add this line, value should match your needs
    # --- END OF CHANGE ---

    # Dynamic gating params
    dynamic_k: 0.95           # CU threshold multiplier
    ce_bias: 0.075             # CE bias term

    # --- START OF OPTIONAL CHANGE (For LoRA parity if needed, won't cause *this* error but good for consistency) ---
    init_prior_from_mlp: false # If you want to enable this, add it here (Qwen models typically don't init from MLP weights directly like Llama)

    lora:
      enable_lora_main_path: true
      enable_lora_prior_ffn: true
      r: 8
      lora_alpha: 16
      lora_dropout: 0.05
      bias: "none"
      # These target modules might need adjustment for Qwen's specific layer names
      lora_target_modules_main: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"] # Check Qwen's actual module names
      lora_target_modules_prior_ffn: ["w1", "w2", "w3"]
    # --- END OF OPTIONAL CHANGE ---

training:
  accumulate_grad_batches: 64
  max_iters: 100000
  eval_interval: 1000

  optimizer:
    # --- START OF CHANGE ---
    base_lr: 1.0e-5       # Max LR for base model
    prior_ffn_lr: 1.0e-5  # Max LR for prior FFN
    # --- END OF CHANGE ---
    weight_decay: 0.01

  scheduler:
    factor: 0.1
    patience: 10
  
  gate_warmup_iters: 100    # Warmup steps for CU

logging:
  tensorboard:
    enabled: true
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"