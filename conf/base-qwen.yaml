defaults:
  - _self_
  - data: sft_mix_qwen

run:
  name: "qwen2.5-0.5B-dynamic-${data.name}-${now:%Y-%m-%d_%H-%M-%S}-gamma=${model.model_cfg.capacity_gamma}-fm=${model.model_cfg.freeze_main_transformer_blocks}"
  output_dir: "outputs/${run.name}"
  seed: 42
  device: "auto"           # "cpu", "cuda", "mps", etc.
  precision: "bf16-mixed"  # "32-true", "16-mixed", "bf16-mixed"
  run_final_evaluation: true

data:
  name: ${hydra:runtime.choices.data}

model:
  _target_: src.trainers.d_qwen_trainer.DynamicQwenTrainer
  model_cfg:
    model_name: "Qwen/Qwen2.5-0.5B-Instruct"

    # Removed: prior_loss_weight (loss is now just LM loss)

    # New Dynamic Gating parameters for VPRRouter
    capacity_gamma: 0.5                     # float in (0,1], fraction of tokens to compute (like MoD's k)
    beta_ce_init: 10.0                       # Initial beta for CE sigmoid (controls steepness)
    beta_cu_init: 10.0                       # Initial beta for CU sigmoid (controls steepness)
    cu_detection_multiplier_init: 1.0        # Initial multiplier for CU norm (replaces old dynamic_k usage)
    ce_criterion_offset_init: 0.075          # Initial offset for CE criterion (NOW A LEARNABLE PARAMETER)

    token_wise_gating: True                  # Whether to apply gating per-token or per-batch
    moving_average_window_size: 100          # Window size for D_st moving average in CU

    # Prior FFN configuration
    prior_ffn_intermediate_size_factor: 2.0  # Factor to multiply hidden_size by for prior FFN intermediate layer (e.g., 2.0 * hidden_size)

    # Training control: Freeze main transformer blocks
    freeze_main_transformer_blocks: False    # If True, only VPR router and Prior FFN params are trained.

training:
  accumulate_grad_batches: 64
  max_iters: 100000
  eval_interval: 1000

  optimizer:
    base_lr: 1.0e-5       # Max LR for main transformer blocks (if not frozen) and other base model params
    prior_ffn_lr: 1.0e-4  # Max LR for prior FFN and VPR router parameters
    weight_decay: 0.01
    warmup_ratio: 0.01 # New: Warmup ratio for LR schedulers (e.g., 1% of max_iters)

  scheduler:
    factor: 0.1
    patience: 10
  
  # Removed: gate_warmup_iters (this logic is now inside VPRRouter and uses current_iter directly)

logging:
  tensorboard:
    enabled: true
  wandb:
    enabled: true
    project: "Dynamic-Transformers"
    entity: "huawei-noahs-ark"