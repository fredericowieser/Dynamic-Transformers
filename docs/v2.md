# Project State Snapshot: Dynamic-Transformers Refactoring

## Overall Goal
Refactor the codebase to streamline the architecture, eliminate redundant code, improve modularity, correct naming conventions, and ensure a clean, robust, and config-driven design across models and data loading.

## Key Knowledge
-   `STD` acronym was corrected to `SDT` throughout the codebase.
-   New core base classes (`DynamicBlock`, `BaseForCausalLM`, `BasePriorNetwork`, `BaseRouter`, `CausalRouter`, `BaseSurpriseRouter`) were established for shared functionality and code reuse.
-   Model-specific logic (routers, prior networks, decision layers) is now encapsulated in `src/models/{mod,sdt,stt}/model.py` files.
-   `...ForCausalLM` classes now inherit from `BaseForCausalLM` and define only unique layer structures and forward passes.
-   `STTTransitionNetwork` includes pre-normalization.
-   `get_trainable_parameters` is used for flexible optimizer setup, with learning rates defined in `config/training/default.yaml` under `optimizer.lrs`.
-   Configuration files (`config/model/default.yaml`, `config/training/default.yaml`) were unified and simplified, with model-specific parameters under a `params` key.
-   A new `STTLayer` encapsulates the complex teacher-student logic for STT models.
-   `SDTRouter` now accepts `beta_ce` and `beta_cu` as `kwargs` for consistency with STT's scheduled betas.
-   The data-loading pipeline was refactored using a `BaseDatasetHandler`, with `HuggingFaceDataset` and `PretrainingDataset` inheriting from it. `MixedDataset` was streamlined to use a dictionary-based mapping for dataset handlers.
-   All `__init__.py` files and file naming conventions were standardized.
-   Indentation errors in `get_trainable_parameters` for `MoDForCausalLM` and `STTForCausalLM` were corrected.
-   Outdated debug line in `train.py` removed.
-   Beta schedule logic in `train.py` refactored.
-   **Crucially**: Model-specific hyperparameters are now passed as a clean Python dictionary (`model_params`) to model constructors, bypassing previous complex and problematic interactions with OmegaConf's `DictConfig` objects and `transformers`' `Qwen2Config` object.

## File System State (Summary of Major Changes)
-   **DELETED**: `src/models/base/dynamic_model.py`, `src/models/base/layers.py`, `src/models/mod/layers.py`, `src/models/mod/routers.py`, `src/models/std` (directory), `src/models/stt/layers.py`, `src/models/stt/priors.py`, `src/models/stt/routers.py`, `src/data/datasets.py`.
-   **RENAMED**: `src/models/std` to `src/models/sdt`, `src/models/standard/causalLM.py` to `src/models/standard/model.py`.
-   **MODIFIED (Key Files)**:
    -   `src/models/base/block.py`: Created `DynamicBlock`.
    -   `src/models/base/causal_lm.py`: Created `BaseForCausalLM`, modified `__init__` to accept `**kwargs` and store `model_params`.
    -   `src/models/base/priors.py`: Created `BasePriorNetwork`, modified `__init__` to accept `**kwargs` and access params from `model_params`.
    -   `src/models/base/routers.py`: Created `BaseRouter`, `CausalRouter`, `BaseSurpriseRouter`, modified `__init__` to accept `**kwargs` and access params from `model_params`, fixed syntax errors, added/removed logging, added `OmegaConf` import.
    -   `src/models/mod/model.py`: Created `MoDRouter`, updated parameter reading.
    -   `src/models/sdt/model.py`: Created `SDTPriorNetwork`, `SDTDecisionLayer`, `SDTRouter`, updated parameter reading and `SDTRouter`'s `__init__` and `forward` methods, fixed `SDTPriorNetwork` and `SDTDecisionLayer` to accept `**kwargs`.
    -   `src/models/stt/model.py`: Created `STTTransitionNetwork`, `STTPredictiveRouter`, `STTLayer`, modified `__init__` to accept `**kwargs` and access params from `model_params`.
    -   `src/models/mod/causal_lm.py`: Rebuilt `MoDForCausalLM`, fixed `get_trainable_parameters` indentation, modified `__init__` to accept `**kwargs` and pass to super, updated router initializations.
    -   `src/models/sdt/causal_lm.py`: Rebuilt `SDTForCausalLM`, fixed `get_trainable_parameters` indentation, modified `__init__` to accept `**kwargs` and pass to super, updated router initializations.
    -   `src/models/stt/causal_lm.py`: Rebuilt `STTForCausalLM`, fixed `get_trainable_parameters` indentation, modified `__init__` to accept `**kwargs` and pass to super, updated router initializations.
    -   `src/training/utils.py`: Updated `create_model` (now passes a clean Python dictionary of parameters), `setup_optimizer_and_scheduler`, removed/added `evaluate_perplexity`, modified `create_model_config` to pass `**kwargs` to model, added/removed logging.
    -   `config/model/default.yaml`: Consolidated model params.
    -   `config/training/default.yaml`: Standardized optimizer config.
    -   `train.py`: Updated beta schedule logic, removed debug line, updated `forward_kwargs`.
    -   `src/data/base_dataset.py`: Created `BaseDatasetHandler`.
    -   `src/data/pretraining_dataset.py`: Refactored to inherit from `BaseDatasetHandler`.
    -   `src/data/huggingface_dataset.py`: Refactored to inherit from `BaseDatasetHandler`.
    -   `src/data/mixed_dataset.py`: Streamlined `setup` method.
    -   `src/data/__init__.py`: Updated imports/exports.
    -   `config/laptop_10m_wikitext.yaml`: Changed `model.type` to `sdt`.
    -   `config/model/10M_scratch.yaml`: Changed `num_hidden_layers` to 2, removed `params: {}`.

## Recent Actions (Focus on Iterative Debugging)
-   Iteratively ran `accelerate launch train.py --config-name laptop_10m_wikitext` to identify and fix errors.
-   Resolved numerous `SyntaxError` and `IndentationError` issues caused by previous `write_file` operations.
-   Addressed `NameError: name 'DictConfig' is not defined` by adding missing `DictConfig` imports to various files.
-   Tackled persistent `AttributeError: 'Qwen2Config' object has no attribute 'params'` and `KeyError: 'sdt_capacity'` by:
    -   Attempting to set `params` directly on `Qwen2Config` (failed due to `transformers` library's strictness).
    -   Trying to use `OmegaConf.to_container` with and without `resolve=False` (failed to resolve `KeyError`).
    -   Finally, implementing a strategy to pass a clean, standard Python dictionary of model-specific parameters (`model_params`) to model constructors, completely bypassing OmegaConf's `DictConfig` objects for these specific parameters.
-   Refactored all model `__init__` methods and router `__init__` methods to accept `**kwargs` and access parameters from `self.model_params`.
-   Added and removed extensive logging statements to pinpoint the exact state of variables during execution.

## Current Status / What's Left
The last error encountered was `TypeError: SDTForCausalLM.__init__() got an unexpected keyword argument 'dtf_capacity'`. This was due to a mismatch in how `create_model` was passing `**model_kwargs` and how `SDTForCausalLM` was accepting them. I have just implemented a fix for this by modifying `SDTForCausalLM.__init__` to accept `**kwargs` and pass them to `super().__init__`.

The next step is to run the command again and see if this fix resolves the `TypeError`. If it does, we will then need to verify that all other parameters are correctly accessed and that the training script runs without further errors. The goal is to achieve a clean run of the training script.
