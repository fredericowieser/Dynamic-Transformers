# Dynamic Transformer (DTF) - Technical Specification

This document provides a technical summary of the Dynamic Transformer (DTF) architecture, its theoretical underpinnings, and its core methodology, based on the research paper "Dynamic Transformers".

---

## 1. Introduction

### 1.1 The Problem with Standard Transformers

The standard Transformer (TF) architecture, while powerful, suffers from a key inefficiency: it applies a uniform amount of computation to every token in a sequence. This design choice leads to significant challenges:

* Scalability: The self-attention mechanism has a computational complexity that scales quadratically with the sequence length, denoted as O(T^2). This becomes a major bottleneck for processing long contexts.
* Redundancy: The same computational effort is spent on semantically critical tokens and common, less informative tokens (e.g., stop words), representing a misallocation of resources.

### 1.2 Conditional Computation as a Solution

To address these issues, conditional computation methods have been developed. These models dynamically allocate computational resources based on the input.

* Mixture-of-Depths (MoD): An engineering-driven approach where a lightweight router selects a fixed number of "important" tokens (Top-K) to be processed by a Transformer block, while the rest bypass it via a residual connection.
* A Principled Alternative: This work proposes a more theoretically grounded paradigm for conditional computation based on the concept of "surprise", drawn from computational neuroscience and information theory. The core idea is that computation should be allocated to tokens that represent the most surprising new information, requiring an update to the model's internal state.

This principle is embodied in the Dynamic Transformer (DTF), an architecture inspired by models like Variational Predictive Routing (VPR).

---

## 2. Background and Theory

### 2.1 Key Concepts

#### Transformer Architecture

The DTF builds upon the standard decoder-only Transformer. Key components include:

* Scaled Dot-Product Attention: The core mechanism for relating tokens in a sequence.
    $$
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
    $$
    where Q, K, V are the query, key, and value matrices, and M is an additive mask for causality.
* Position-wise Feed-Forward Networks (FFNs): Applied to each token representation independently. Modern TFs often use gated variants like SwiGLU.
* Residual Connections and Normalization: Essential for training deep networks and stabilizing gradient flow.

#### Bayesian Surprise and Predictive Coding

* Predictive Coding: A theory suggesting the brain minimizes prediction error by comparing top-down predictions with bottom-up sensory input.
* Bayesian Surprise: Quantifies the degree of belief update after observing new data. It's measured by the Kullback-Leibler (KL) divergence between the posterior belief (q) and the prior belief (p): D_{KL}(q || p).
* Variational Predictive Routing (VPR): A hierarchical model that uses surprise to dynamically gate information flow. It evaluates two competing hypotheses at each time step:
    1.  Static Hypothesis: The state remains unchanged. The surprise is D_{st}.
    2.  Change Hypothesis: The state evolves according to a learned predictive model. The surprise is D_{ch}.
* VPR Event Criteria: An "event" (i.e., a state update) is triggered if:
    * Expected Change (CE): D_{st} > D_{ch}. The learned model's prediction was better than assuming no change.
    * Unexpected Change (CU): D_{st} exceeds a moving average of recent surprise. An unforeseen change occurred.

The DTF translates these temporal, event-driven principles into the token-wise domain of the Transformer.

---

## 3. The Dynamic Transformer (DTF) Methodology

The DTF modifies a standard pre-trained decoder-only Transformer by replacing its uniform stack of layers with alternating Decision Layers and Dynamic Layers.

### 3.1 The Decision Layer

The Decision Layer's purpose is to generate, for each token, the three vector representations required for the subsequent surprise-based routing decision. Given an input hidden state tensor H^{(l)} \in \mathbb{R}^{B \times T \times d}, the layer computes:

1.  Original State (H_{orig}^{(l)}): The unmodified input, representing the baseline for the static hypothesis.
    $$
    H_{orig}^{(l)} = H^{(l)}
    $$
   
2.  Posterior State (H_{post}^{(l)}): The output of a standard, full Transformer block. This represents the "ground truth" updated state for the layer.
    $$
    H_{post}^{(l)} = H^{(l)} + \text{TF-Block}(H^{(l)})
    $$
   
3.  Prior State (H_{prior}^{(l)}): A computationally cheap prediction of the posterior state, generated by a small PriorFFN. This represents the change hypothesis.
    $$
    H_{prior}^{(l)} = H^{(l)} + \text{PriorFFN}(\text{RMSNorm}(H^{(l)}))
    $$

#### PriorFFN and Auxiliary Loss

The PriorFFN is a lightweight network (e.g., a SwiGLU with a small intermediate dimension) that learns to approximate the transformation of the full TF block. It is trained with an auxiliary Mean Squared Error (MSE) loss:
$$\mathcal{L}_{\text{prior}} = \text{MSE}(H_{prior}^{(l)}, \text{stop\_gradient}(H_{post}^{(l)}))$$

This loss is added to the main language modeling objective with a small weighting factor \lambda_{prior}.

### 3.2 The Dynamic Layer and Predictive Router

The Dynamic Layer receives the states from the Decision Layer and performs the conditional computation, governed by the Predictive Router.

#### 1. Surprise Calculation

The router first computes two token-wise "surprise" metrics using MSE as a computationally efficient proxy for KL divergence. For each token i:

* Static Surprise (D_{st,i}): The error of the static hypothesis (no change).
    $$
    D_{st,i} = \frac{1}{d} ||H_{post,i}^{(l)} - H_{orig,i}^{(l)}||_2^2
    $$
   
* Change Surprise (D_{ch,i}): The error of the change hypothesis (PriorFFN prediction).
    $$
    D_{ch,i} = \frac{1}{d} ||H_{post,i}^{(l)} - H_{prior,i}^{(l)}||_2^2
    $$
   

#### 2. Gating Criteria

Next, the router applies soft, differentiable versions of the VPR criteria to generate gating signals. These criteria involve four learnable parameters: an offset o_{ce}, a multiplier m_{cu}, and two inverse temperatures \beta_{ce}, \beta_{cu}.

* Criterion E (Expected): Measures if the PriorFFN's prediction is a better fit than the original state.
    $$
    CE_i = D_{st,i} - (D_{ch,i} - \log(o_{ce} + \epsilon))
    $$
   
* Criterion U (Unexpected): Measures if the static surprise significantly exceeds its recent history (a moving average, MA).
    $$
    CU_i = D_{st,i} - (m_{cu} \cdot \text{MA}(D_{st,i}))
    $$
   

#### 3. Differentiable Routing

The raw CE and CU scores are converted into probabilities using scaled sigmoid functions. The inverse temperatures are passed through a Softplus function to ensure they remain positive.

$$S_{CE} = \sigma(\text{Softplus}(\beta_{ce}) \cdot CE)$$

$$S_{CU} = \sigma(\text{Softplus}(\beta_{cu}) \cdot CU)$$


These probabilities are combined using a probabilistic OR to form a final continuous gating signal G_{cont} \in [0, 1]^T:
$$G_{cont} = S_{CE} + S_{CU} - (S_{CE} \cdot S_{CU})$$


#### 4. State Update

A fixed capacity of tokens, \gamma \cdot T, with the highest G_{cont} scores are selected for further computation (Top-K). Let \mathcal{S} be the set of selected token indices. The final state update is performed via a gated residual connection:
$$
H_i^{(l+1)} =
\begin{cases}
    H_{post, i}^{(l)} + G_{cont, i} \cdot \text{TF-Block}(H_{post, i}^{(l)}) & \text{if } i \in \mathcal{S} \\
    H_{post, i}^{(l)} & \text{if } i \notin \mathcal{S}
\end{cases}
$$

Tokens not selected bypass the second TF block, saving computation, while still carrying forward the update from the Decision Layer's block. The sequence is then re-merged and passed to the next Decision Layer.