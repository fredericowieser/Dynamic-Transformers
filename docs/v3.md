---

## Current Debugging Session Log

**Issue:** `TypeError: 'NoneType' object is not subscriptable` at `out["logits"].shape` in smoke test.
**Analysis:** Model's `forward` method (or a sub-method) is returning `None` or an incomplete dictionary.
**Hypothesis:** `STTForCausalLM._forward_layers` or `STTLayer.forward` might have incomplete return paths.

**Action:** Add debug prints to `BaseForCausalLM.forward` to inspect `final_outputs` before return.
**Result:** `replace` tool failed because the `forward` method was truncated.

**Action:** Manually restore `src/models/base/causal_lm.py` to its full original content from `Dynamic-Transformers_V2_Functioning.md`.
**Result:** File restored.

**Action:** Re-apply the patch to `src/models/base/causal_lm.py`.
**Result:** Patch applied successfully.

**Action:** Re-run smoke test.
**Result:** `NameError: name 'PreTrainedModel' is not defined`.

**Issue:** `NameError: name 'PreTrainedModel' is not defined`.
**Analysis:** `BaseForCausalLM` inherits from `PreTrainedModel`, but `PreTrainedModel` was not imported.
**Action:** Added `from transformers import PreTrainedModel` to `src/models/base/causal_lm.py`.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** `RuntimeError: The size of tensor a (8) must match the size of tensor b (0) at non-singleton dimension 3` in `custom_rotary_embedding.py`.

**Issue:** `RuntimeError: The size of tensor a (8) must match the size of tensor b (0) at non-singleton dimension 3` in `custom_rotary_embedding.py`.
**Analysis:** This indicated a dimension mismatch between query states (`q`) and `cos`/`sin` tensors. Specifically, `cos`/`sin` had a dimension of size 0. This happened because `Qwen2RotaryEmbedding` was initialized with `hidden_size=config.head_dim` (8), but it expects the full `config.hidden_size` (32) to correctly calculate its internal `inv_freq` and thus `cos`/`sin` tensors.
**Action:** Reverted `hidden_size` in `rope_cfg` (passed to `Qwen2RotaryEmbedding`) from `config.head_dim` back to `config.hidden_size` in `BaseForCausalLM.__init__`.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** `omegaconf.errors.ConfigKeyError: Missing key prior_ffn_intermediate_size_factor`.

**Issue:** `omegaconf.errors.ConfigKeyError: Missing key prior_ffn_intermediate_size_factor`.
**Analysis:** `prior_ffn_intermediate_size_factor` was expected in `model_cfg` but was not present due to how `temp_smoke_test.py` loaded the config.
**Action:** Modified `temp_smoke_test.py` to use `hydra.initialize` and `hydra.compose` for proper config loading.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** `omegaconf.errors.ConfigAttributeError: Missing key component`.

**Issue:** `omegaconf.errors.ConfigAttributeError: Missing key component`.
**Analysis:** `cfg.component` was being accessed directly, but `cfg` did not have a top-level `component` key in the composed config.
**Action:** Reverted the previous change in `src/training/utils.py` to avoid merging `cfg.component.prior` into `model_cfg` in `create_model`.
**Result:** `Successfully modified file`.

**Action:** Explicitly set `cfg.model.prior_ffn_intermediate_size_factor = 0.25` in `temp_smoke_test.py`.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** `omegaconf.errors.ConfigKeyError: Key 'stt_capacity' is not in struct`.

**Issue:** `omegaconf.errors.ConfigKeyError: Key 'stt_capacity' is not in struct`.
**Analysis:** `stt_capacity` was expected in `model_cfg` but was not present.
**Action:** Explicitly set `cfg.model.stt_capacity = 0.5` in `temp_smoke_test.py`.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** `omegaconf.errors.ConfigAttributeError: Key 'stt_capacity' is not in struct` (due to `DictConfig` being structured).

**Issue:** `omegaconf.errors.ConfigAttributeError: Key 'stt_capacity' is not in struct`.
**Analysis:** `DictConfig` objects are structured by default and do not allow adding new keys dynamically.
**Action:** Disabled strict mode for `cfg.model` using `OmegaConf.set_struct(cfg.model, False)` in `temp_smoke_test.py`.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** `RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1`.

**Issue:** `RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1`.
**Analysis:** `Qwen2RotaryEmbedding` was being called with `hidden_states` (shape `[bs, seq_len, hidden_size]`) instead of `query_states` (shape `[bs, num_heads, seq_len, head_dim]`). Also, `cos` and `sin` were not being returned with the correct `num_heads` dimension by `Qwen2RotaryEmbedding` (or were being squeezed).
**Action:**
    1.  Removed `self.rotary_emb` from `BaseForCausalLM.__init__` and `forward`.
    2.  Added `self.rotary_emb = Qwen2RotaryEmbedding(config)` to `CustomQwen2Attention.__init__`.
    3.  Called `cos, sin = self.rotary_emb(query_states, position_ids)` in `CustomQwen2Attention.forward`.
    4.  Explicitly `unsqueeze(1)` `cos` and `sin` after they are returned from `self.rotary_emb`.
**Result:** `Successfully modified file` for each step.

**Action:** Added `logging` import and `log` object initialization to `src/models/base/custom_attention.py` and `src/models/stt/model.py` to enable debug prints.
**Result:** `Successfully modified file` for each step.

**Action:** Set logging level to `DEBUG` in `temp_smoke_test.py` to view debug prints.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test.
**Result:** **Smoke test passed for STT!** `OK torch.Size([2, 64, 151936]) 31.288686752319336`.

**Issue:** `TypeError: Can't instantiate abstract class MoDRouter with abstract method forward`.
**Analysis:** `MoDRouter`'s `forward` method signature did not exactly match the abstract `forward` method in `BaseRouter`.
**Action:** Modified `MoDRouter.forward` signature to `def forward(self, *args, **kwargs)` and unpacked `hidden_states` from `args`.
**Result:** `Successfully modified file`.

**Issue:** `OSError: Qwen/Qwen2.5-10M is not a local folder` for the standard model.
**Analysis:** The `StandardTransformerForCausalLM.from_pretrained` was being called unconditionally, even when `from_scratch=True`.
**Action:** Modified `src/training/utils.py` to initialize `StandardTransformerForCausalLM` directly from `config` when `from_scratch=True`.
**Result:** `Successfully modified file`.

**Issue:** `TypeError: StandardTransformerForCausalLM.__init__() got an unexpected keyword argument 'model_cfg'`.
**Analysis:** `StandardTransformerForCausalLM.__init__` does not accept a `model_cfg` argument.
**Action:** Removed the `model_cfg=cfg.model` argument when initializing `StandardTransformerForCausalLM` from scratch.
**Result:** `Successfully modified file`.

**Action:** Re-run smoke test for all model types.
**Result:** **All four model types (STT, SDT, MOD, STANDARD) passed the smoke test!**

**Resolution:** The combination of correctly initializing `Qwen2RotaryEmbedding` within `CustomQwen2Attention` and ensuring `cos` and `sin` tensors have the correct `num_heads` dimension (via `unsqueeze(1)`) resolved the `RuntimeError`. The `ConfigKeyError` and `ConfigAttributeError` were resolved by correctly configuring `temp_smoke_test.py` to load Hydra configs and dynamically add parameters. The `TypeError: Can't instantiate abstract class MoDRouter` was resolved by aligning the `MoDRouter.forward` signature with its abstract base. The `OSError` and `TypeError` for the standard model were resolved by correctly handling `from_scratch` initialization.

**Final State of `temp_smoke_test.py`:** The `temp_smoke_test.py` file is kept in its state that tests all four model types, including the necessary `OmegaConf.set_struct(cfg.model, False)` and model-specific parameter settings. The logging imports in `src/models/base/routers.py` and `src/models/base/custom_attention.py` and `src/models/stt/model.py` are also retained to ensure the smoke test runs without `NameError`.